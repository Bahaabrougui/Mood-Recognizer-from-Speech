{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import librosa\n",
    "import librosa.display\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from matplotlib.pyplot import specgram\n",
    "import keras\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Input, Flatten, Dropout, Activation\n",
    "from keras.layers import Conv1D, MaxPooling1D, AveragePooling1D\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from keras import regularizers\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "mylist= os.listdir('C:\\\\Users\\Kalelt\\'has\\Desktop\\Stage\\Sound Data\\Raw Data')\n",
    "type(mylist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03-01-01-01-01-01-11.wav\n"
     ]
    }
   ],
   "source": [
    "print(mylist[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "su\n"
     ]
    }
   ],
   "source": [
    "print(mylist[2800][:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2812"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mylist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22050"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data, sampling_rate = librosa.load('C:\\\\Users\\Kalelt\\'has\\Desktop\\Stage\\Sound Data\\Raw Data\\\\03-02-02-01-01-01-01.wav')\n",
    "sampling_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PolyCollection at 0x1c3ea3e4d90>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import glob\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "librosa.display.waveplot(data, sr=sampling_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "<ipython-input-1-038f054dadcd>:6: WavFileWarning: Chunk (non-data) not understood, skipping it.\n  sr,x = scipy.io.wavfile.read('C:\\\\Users\\Kalelt\\'has\\Desktop\\Stage\\Sound Data\\Raw Data\\\\03-02-02-01-01-01-01.wav')\n<ipython-input-1-038f054dadcd>:19: RuntimeWarning: divide by zero encountered in log\n  X[i,:] = np.log(np.abs(z[:nfft//2]))\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import scipy.io.wavfile\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "sr,x = scipy.io.wavfile.read('C:\\\\Users\\Kalelt\\'has\\Desktop\\Stage\\Sound Data\\Raw Data\\\\03-02-02-01-01-01-01.wav')\n",
    "\n",
    "nstep = int(sr * 0.01)\n",
    "nwin  = int(sr * 0.03)\n",
    "nfft = nwin\n",
    "\n",
    "window = np.hamming(nwin)\n",
    "\n",
    "nn = range(nwin, len(x), nstep)\n",
    "X = np.zeros( (len(nn), nfft//2))\n",
    "for i,n in enumerate(nn):\n",
    "    xseg = x[n-nwin:n]\n",
    "    z = np.fft.fft(window * xseg, nfft)\n",
    "    X[i,:] = np.log(np.abs(z[:nfft//2]))\n",
    "\n",
    "plt.imshow(X.T, interpolation='nearest',\n",
    "    origin='lower',\n",
    "    aspect='auto')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "feeling_list=[]\n",
    "for item in mylist:\n",
    "    if item[6:-16]=='02' and int(item[18:-4])%2==0:\n",
    "        feeling_list.append('female_calm')\n",
    "    elif item[6:-16]=='02' and int(item[18:-4])%2==1:\n",
    "        feeling_list.append('male_calm')\n",
    "    elif item[6:-16]=='03' and int(item[18:-4])%2==0:\n",
    "        feeling_list.append('female_happy')\n",
    "    elif item[6:-16]=='03' and int(item[18:-4])%2==1:\n",
    "        feeling_list.append('male_happy')\n",
    "    elif item[6:-16]=='04' and int(item[18:-4])%2==0:\n",
    "        feeling_list.append('female_sad')\n",
    "    elif item[6:-16]=='04' and int(item[18:-4])%2==1:\n",
    "        feeling_list.append('male_sad')\n",
    "    elif item[6:-16]=='05' and int(item[18:-4])%2==0:\n",
    "        feeling_list.append('female_angry')\n",
    "    elif item[6:-16]=='05' and int(item[18:-4])%2==1:\n",
    "        feeling_list.append('male_angry')\n",
    "    elif item[6:-16]=='06' and int(item[18:-4])%2==0:\n",
    "        feeling_list.append('female_fearful')\n",
    "    elif item[6:-16]=='06' and int(item[18:-4])%2==1:\n",
    "        feeling_list.append('male_fearful')\n",
    "#     elif item[6:-16]=='07' and int(item[18:-4])%2==0:\n",
    "#         feeling_list.append('female_disgust')\n",
    "#     elif item[6:-16]=='07' and int(item[18:-4])%2==1:\n",
    "#         feeling_list.append('male_disgust')\n",
    "#     elif item[6:-16]=='08' and int(item[18:-4])%2==0:\n",
    "#         feeling_list.append('female_surprised')\n",
    "#     elif item[6:-16]=='08' and int(item[18:-4])%2==1:\n",
    "#         feeling_list.append('male_surprised')\n",
    "#     elif item[6:-16]=='01' and int(item[18:-4])%2==0:\n",
    "#         feeling_list.append('female_neutral')\n",
    "#     elif item[6:-16]=='01' and int(item[18:-4])%2==1:\n",
    "#         feeling_list.append('male_neutral')\n",
    "    elif item[:1]=='a':\n",
    "        feeling_list.append('male_angry')\n",
    "    elif item[:1]=='f':\n",
    "        feeling_list.append('male_fearful')\n",
    "    elif item[:1]=='h':\n",
    "        feeling_list.append('male_happy')\n",
    "#     elif item[:1]=='n':\n",
    "#         feeling_list.append('male_neutral')\n",
    "#     elif item[:1]=='d':\n",
    "#         feeling_list.append('male_disgust')\n",
    "#     elif item[:2]=='su':\n",
    "#         feeling_list.append('male_surprised')\n",
    "    elif item[:2]=='sa':\n",
    "        feeling_list.append('male_sad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>male_calm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>female_calm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>male_calm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>female_calm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>male_calm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2115</th>\n",
       "      <td>male_sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2116</th>\n",
       "      <td>male_sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2117</th>\n",
       "      <td>male_sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2118</th>\n",
       "      <td>male_sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2119</th>\n",
       "      <td>male_sad</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2120 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            label\n",
       "0       male_calm\n",
       "1     female_calm\n",
       "2       male_calm\n",
       "3     female_calm\n",
       "4       male_calm\n",
       "...           ...\n",
       "2115     male_sad\n",
       "2116     male_sad\n",
       "2117     male_sad\n",
       "2118     male_sad\n",
       "2119     male_sad\n",
       "\n",
       "[2120 rows x 1 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = pd.DataFrame(feeling_list)\n",
    "labels = labels.rename(columns={0: 'label'})\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=['feature'])\n",
    "bookmark=0\n",
    "for index,y in enumerate(mylist):\n",
    "    if mylist[index][6:-16]!='01' and mylist[index][6:-16]!='07' and mylist[index][6:-16]!='08' and mylist[index][:2]!='su' and mylist[index][:1]!='n' and mylist[index][:1]!='d':\n",
    "        X, sample_rate = librosa.load('C:\\\\Users\\Kalelt\\'has\\Desktop\\Stage\\Sound Data\\Raw Data\\\\'+y, res_type='kaiser_fast',duration=2.5,sr=22050*2,offset=0.5)\n",
    "        sample_rate = np.array(sample_rate)\n",
    "        mfccs = np.mean(librosa.feature.mfcc(y=X,\n",
    "                                            sr=sample_rate,\n",
    "                                            n_mfcc=13),\n",
    "                        axis=0)\n",
    "        feature = mfccs\n",
    "        df.loc[bookmark] = [feature]\n",
    "        bookmark=bookmark+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[-70.26777, -70.26777, -70.26777, -70.26777, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[-65.70765, -65.70765, -63.11472, -61.518997, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[-65.4825, -65.4825, -65.4825, -65.4825, -65.4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[-64.52845, -64.52845, -64.52845, -64.52845, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[-62.36431, -59.934727, -61.869602, -67.49577,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             feature\n",
       "0  [-70.26777, -70.26777, -70.26777, -70.26777, -...\n",
       "1  [-65.70765, -65.70765, -63.11472, -61.518997, ...\n",
       "2  [-65.4825, -65.4825, -65.4825, -65.4825, -65.4...\n",
       "3  [-64.52845, -64.52845, -64.52845, -64.52845, -...\n",
       "4  [-62.36431, -59.934727, -61.869602, -67.49577,..."
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2120"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3 = pd.DataFrame(df['feature'].values.tolist())\n",
    "bookmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "newdf = pd.concat([df3,labels], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>207</th>\n",
       "      <th>208</th>\n",
       "      <th>209</th>\n",
       "      <th>210</th>\n",
       "      <th>211</th>\n",
       "      <th>212</th>\n",
       "      <th>213</th>\n",
       "      <th>214</th>\n",
       "      <th>215</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-70.267769</td>\n",
       "      <td>-70.267769</td>\n",
       "      <td>-70.267769</td>\n",
       "      <td>-70.267769</td>\n",
       "      <td>-70.267769</td>\n",
       "      <td>-70.267769</td>\n",
       "      <td>-70.267769</td>\n",
       "      <td>-70.267769</td>\n",
       "      <td>-70.267769</td>\n",
       "      <td>-70.267769</td>\n",
       "      <td>...</td>\n",
       "      <td>-57.447464</td>\n",
       "      <td>-58.896500</td>\n",
       "      <td>-58.750996</td>\n",
       "      <td>-57.405678</td>\n",
       "      <td>-60.078484</td>\n",
       "      <td>-63.426800</td>\n",
       "      <td>-62.638542</td>\n",
       "      <td>-61.082737</td>\n",
       "      <td>-60.234661</td>\n",
       "      <td>male_calm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-65.707649</td>\n",
       "      <td>-65.707649</td>\n",
       "      <td>-63.114719</td>\n",
       "      <td>-61.518997</td>\n",
       "      <td>-61.097141</td>\n",
       "      <td>-63.424599</td>\n",
       "      <td>-63.720066</td>\n",
       "      <td>-56.854614</td>\n",
       "      <td>-55.168972</td>\n",
       "      <td>-54.639999</td>\n",
       "      <td>...</td>\n",
       "      <td>-39.792141</td>\n",
       "      <td>-40.613159</td>\n",
       "      <td>-41.209202</td>\n",
       "      <td>-41.439201</td>\n",
       "      <td>-43.994278</td>\n",
       "      <td>-49.399620</td>\n",
       "      <td>-50.591599</td>\n",
       "      <td>-49.144051</td>\n",
       "      <td>-48.705654</td>\n",
       "      <td>female_calm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-65.482498</td>\n",
       "      <td>-65.482498</td>\n",
       "      <td>-65.482498</td>\n",
       "      <td>-65.482498</td>\n",
       "      <td>-65.482498</td>\n",
       "      <td>-65.482498</td>\n",
       "      <td>-65.482498</td>\n",
       "      <td>-65.482498</td>\n",
       "      <td>-65.482498</td>\n",
       "      <td>-65.482498</td>\n",
       "      <td>...</td>\n",
       "      <td>-31.346558</td>\n",
       "      <td>-34.310772</td>\n",
       "      <td>-35.800705</td>\n",
       "      <td>-35.936115</td>\n",
       "      <td>-37.631844</td>\n",
       "      <td>-40.119411</td>\n",
       "      <td>-41.662888</td>\n",
       "      <td>-41.323643</td>\n",
       "      <td>-40.710770</td>\n",
       "      <td>male_calm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-64.528450</td>\n",
       "      <td>-64.528450</td>\n",
       "      <td>-64.528450</td>\n",
       "      <td>-64.528450</td>\n",
       "      <td>-64.528450</td>\n",
       "      <td>-64.528450</td>\n",
       "      <td>-64.528450</td>\n",
       "      <td>-64.528450</td>\n",
       "      <td>-64.528450</td>\n",
       "      <td>-65.928223</td>\n",
       "      <td>...</td>\n",
       "      <td>-48.674301</td>\n",
       "      <td>-48.596073</td>\n",
       "      <td>-47.602745</td>\n",
       "      <td>-43.049198</td>\n",
       "      <td>-42.659542</td>\n",
       "      <td>-43.188560</td>\n",
       "      <td>-44.001244</td>\n",
       "      <td>-43.610100</td>\n",
       "      <td>-44.698246</td>\n",
       "      <td>female_calm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-62.364311</td>\n",
       "      <td>-59.934727</td>\n",
       "      <td>-61.869602</td>\n",
       "      <td>-67.495773</td>\n",
       "      <td>-71.071808</td>\n",
       "      <td>-65.679817</td>\n",
       "      <td>-63.394402</td>\n",
       "      <td>-65.503349</td>\n",
       "      <td>-61.856644</td>\n",
       "      <td>-60.005428</td>\n",
       "      <td>...</td>\n",
       "      <td>-39.071327</td>\n",
       "      <td>-41.897121</td>\n",
       "      <td>-40.865437</td>\n",
       "      <td>-38.290604</td>\n",
       "      <td>-36.372398</td>\n",
       "      <td>-37.915779</td>\n",
       "      <td>-40.026127</td>\n",
       "      <td>-43.383774</td>\n",
       "      <td>-43.965401</td>\n",
       "      <td>male_calm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2115</th>\n",
       "      <td>-31.908884</td>\n",
       "      <td>-29.785275</td>\n",
       "      <td>-29.312561</td>\n",
       "      <td>-29.340696</td>\n",
       "      <td>-29.309517</td>\n",
       "      <td>-31.234791</td>\n",
       "      <td>-31.892527</td>\n",
       "      <td>-29.230808</td>\n",
       "      <td>-26.412685</td>\n",
       "      <td>-25.962688</td>\n",
       "      <td>...</td>\n",
       "      <td>-28.468334</td>\n",
       "      <td>-29.646355</td>\n",
       "      <td>-28.075077</td>\n",
       "      <td>-28.881575</td>\n",
       "      <td>-28.615490</td>\n",
       "      <td>-28.692484</td>\n",
       "      <td>-28.945385</td>\n",
       "      <td>-26.115368</td>\n",
       "      <td>-24.266727</td>\n",
       "      <td>male_sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2116</th>\n",
       "      <td>-25.501789</td>\n",
       "      <td>-26.071095</td>\n",
       "      <td>-28.139214</td>\n",
       "      <td>-28.083109</td>\n",
       "      <td>-29.027153</td>\n",
       "      <td>-29.084227</td>\n",
       "      <td>-29.444826</td>\n",
       "      <td>-28.174282</td>\n",
       "      <td>-26.825203</td>\n",
       "      <td>-25.776651</td>\n",
       "      <td>...</td>\n",
       "      <td>-28.675123</td>\n",
       "      <td>-28.234779</td>\n",
       "      <td>-28.040579</td>\n",
       "      <td>-28.183098</td>\n",
       "      <td>-27.638083</td>\n",
       "      <td>-28.436066</td>\n",
       "      <td>-29.349464</td>\n",
       "      <td>-31.348232</td>\n",
       "      <td>-32.599976</td>\n",
       "      <td>male_sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2117</th>\n",
       "      <td>-25.907646</td>\n",
       "      <td>-25.869795</td>\n",
       "      <td>-27.495556</td>\n",
       "      <td>-26.596586</td>\n",
       "      <td>-26.451052</td>\n",
       "      <td>-26.436209</td>\n",
       "      <td>-26.418201</td>\n",
       "      <td>-24.776852</td>\n",
       "      <td>-24.386248</td>\n",
       "      <td>-24.525984</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>male_sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2118</th>\n",
       "      <td>-43.516396</td>\n",
       "      <td>-43.361576</td>\n",
       "      <td>-42.073490</td>\n",
       "      <td>-37.939419</td>\n",
       "      <td>-31.907127</td>\n",
       "      <td>-29.823292</td>\n",
       "      <td>-29.288361</td>\n",
       "      <td>-30.038612</td>\n",
       "      <td>-30.165737</td>\n",
       "      <td>-28.224495</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>male_sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2119</th>\n",
       "      <td>-42.751133</td>\n",
       "      <td>-43.302856</td>\n",
       "      <td>-40.892136</td>\n",
       "      <td>-40.429176</td>\n",
       "      <td>-41.753841</td>\n",
       "      <td>-41.230766</td>\n",
       "      <td>-34.597507</td>\n",
       "      <td>-28.640125</td>\n",
       "      <td>-27.143658</td>\n",
       "      <td>-26.150391</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>male_sad</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2120 rows Ã— 217 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0          1          2          3          4          5  \\\n",
       "0    -70.267769 -70.267769 -70.267769 -70.267769 -70.267769 -70.267769   \n",
       "1    -65.707649 -65.707649 -63.114719 -61.518997 -61.097141 -63.424599   \n",
       "2    -65.482498 -65.482498 -65.482498 -65.482498 -65.482498 -65.482498   \n",
       "3    -64.528450 -64.528450 -64.528450 -64.528450 -64.528450 -64.528450   \n",
       "4    -62.364311 -59.934727 -61.869602 -67.495773 -71.071808 -65.679817   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "2115 -31.908884 -29.785275 -29.312561 -29.340696 -29.309517 -31.234791   \n",
       "2116 -25.501789 -26.071095 -28.139214 -28.083109 -29.027153 -29.084227   \n",
       "2117 -25.907646 -25.869795 -27.495556 -26.596586 -26.451052 -26.436209   \n",
       "2118 -43.516396 -43.361576 -42.073490 -37.939419 -31.907127 -29.823292   \n",
       "2119 -42.751133 -43.302856 -40.892136 -40.429176 -41.753841 -41.230766   \n",
       "\n",
       "              6          7          8          9  ...        207        208  \\\n",
       "0    -70.267769 -70.267769 -70.267769 -70.267769  ... -57.447464 -58.896500   \n",
       "1    -63.720066 -56.854614 -55.168972 -54.639999  ... -39.792141 -40.613159   \n",
       "2    -65.482498 -65.482498 -65.482498 -65.482498  ... -31.346558 -34.310772   \n",
       "3    -64.528450 -64.528450 -64.528450 -65.928223  ... -48.674301 -48.596073   \n",
       "4    -63.394402 -65.503349 -61.856644 -60.005428  ... -39.071327 -41.897121   \n",
       "...         ...        ...        ...        ...  ...        ...        ...   \n",
       "2115 -31.892527 -29.230808 -26.412685 -25.962688  ... -28.468334 -29.646355   \n",
       "2116 -29.444826 -28.174282 -26.825203 -25.776651  ... -28.675123 -28.234779   \n",
       "2117 -26.418201 -24.776852 -24.386248 -24.525984  ...        NaN        NaN   \n",
       "2118 -29.288361 -30.038612 -30.165737 -28.224495  ...        NaN        NaN   \n",
       "2119 -34.597507 -28.640125 -27.143658 -26.150391  ...        NaN        NaN   \n",
       "\n",
       "            209        210        211        212        213        214  \\\n",
       "0    -58.750996 -57.405678 -60.078484 -63.426800 -62.638542 -61.082737   \n",
       "1    -41.209202 -41.439201 -43.994278 -49.399620 -50.591599 -49.144051   \n",
       "2    -35.800705 -35.936115 -37.631844 -40.119411 -41.662888 -41.323643   \n",
       "3    -47.602745 -43.049198 -42.659542 -43.188560 -44.001244 -43.610100   \n",
       "4    -40.865437 -38.290604 -36.372398 -37.915779 -40.026127 -43.383774   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "2115 -28.075077 -28.881575 -28.615490 -28.692484 -28.945385 -26.115368   \n",
       "2116 -28.040579 -28.183098 -27.638083 -28.436066 -29.349464 -31.348232   \n",
       "2117        NaN        NaN        NaN        NaN        NaN        NaN   \n",
       "2118        NaN        NaN        NaN        NaN        NaN        NaN   \n",
       "2119        NaN        NaN        NaN        NaN        NaN        NaN   \n",
       "\n",
       "            215        label  \n",
       "0    -60.234661    male_calm  \n",
       "1    -48.705654  female_calm  \n",
       "2    -40.710770    male_calm  \n",
       "3    -44.698246  female_calm  \n",
       "4    -43.965401    male_calm  \n",
       "...         ...          ...  \n",
       "2115 -24.266727     male_sad  \n",
       "2116 -32.599976     male_sad  \n",
       "2117        NaN     male_sad  \n",
       "2118        NaN     male_sad  \n",
       "2119        NaN     male_sad  \n",
       "\n",
       "[2120 rows x 217 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnewdf = newdf\n",
    "rnewdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>207</th>\n",
       "      <th>208</th>\n",
       "      <th>209</th>\n",
       "      <th>210</th>\n",
       "      <th>211</th>\n",
       "      <th>212</th>\n",
       "      <th>213</th>\n",
       "      <th>214</th>\n",
       "      <th>215</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1009</th>\n",
       "      <td>-59.283310</td>\n",
       "      <td>-59.283310</td>\n",
       "      <td>-59.283310</td>\n",
       "      <td>-59.283310</td>\n",
       "      <td>-59.283310</td>\n",
       "      <td>-59.283310</td>\n",
       "      <td>-59.283310</td>\n",
       "      <td>-60.162197</td>\n",
       "      <td>-58.328056</td>\n",
       "      <td>-58.705463</td>\n",
       "      <td>...</td>\n",
       "      <td>-42.280170</td>\n",
       "      <td>-37.908653</td>\n",
       "      <td>-37.050064</td>\n",
       "      <td>-38.493828</td>\n",
       "      <td>-38.409847</td>\n",
       "      <td>-40.297588</td>\n",
       "      <td>-40.275467</td>\n",
       "      <td>-26.389082</td>\n",
       "      <td>-18.184639</td>\n",
       "      <td>female_calm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1101</th>\n",
       "      <td>-47.759697</td>\n",
       "      <td>-51.465084</td>\n",
       "      <td>-57.384605</td>\n",
       "      <td>-57.002815</td>\n",
       "      <td>-58.422222</td>\n",
       "      <td>-58.849869</td>\n",
       "      <td>-57.612453</td>\n",
       "      <td>-56.452694</td>\n",
       "      <td>-53.982033</td>\n",
       "      <td>-51.257492</td>\n",
       "      <td>...</td>\n",
       "      <td>-37.422802</td>\n",
       "      <td>-37.302223</td>\n",
       "      <td>-38.155952</td>\n",
       "      <td>-40.875015</td>\n",
       "      <td>-41.355923</td>\n",
       "      <td>-40.960178</td>\n",
       "      <td>-41.197647</td>\n",
       "      <td>-31.651970</td>\n",
       "      <td>-23.466846</td>\n",
       "      <td>female_calm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1560</th>\n",
       "      <td>-44.249050</td>\n",
       "      <td>-47.272362</td>\n",
       "      <td>-48.930332</td>\n",
       "      <td>-49.305489</td>\n",
       "      <td>-48.988178</td>\n",
       "      <td>-48.861511</td>\n",
       "      <td>-50.218369</td>\n",
       "      <td>-49.586102</td>\n",
       "      <td>-48.696918</td>\n",
       "      <td>-48.407051</td>\n",
       "      <td>...</td>\n",
       "      <td>-21.699717</td>\n",
       "      <td>-19.994274</td>\n",
       "      <td>-19.988308</td>\n",
       "      <td>-18.959019</td>\n",
       "      <td>-19.945028</td>\n",
       "      <td>-19.368382</td>\n",
       "      <td>-19.366259</td>\n",
       "      <td>-16.705296</td>\n",
       "      <td>-12.413276</td>\n",
       "      <td>male_angry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>-58.587570</td>\n",
       "      <td>-56.556194</td>\n",
       "      <td>-56.574001</td>\n",
       "      <td>-53.165684</td>\n",
       "      <td>-52.330986</td>\n",
       "      <td>-55.095329</td>\n",
       "      <td>-56.259312</td>\n",
       "      <td>-54.002625</td>\n",
       "      <td>-52.834641</td>\n",
       "      <td>-54.240810</td>\n",
       "      <td>...</td>\n",
       "      <td>-55.064278</td>\n",
       "      <td>-54.165936</td>\n",
       "      <td>-52.863503</td>\n",
       "      <td>-50.170200</td>\n",
       "      <td>-50.618374</td>\n",
       "      <td>-50.470371</td>\n",
       "      <td>-50.961624</td>\n",
       "      <td>-53.676678</td>\n",
       "      <td>-54.448097</td>\n",
       "      <td>male_calm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1969</th>\n",
       "      <td>-22.843060</td>\n",
       "      <td>-23.030672</td>\n",
       "      <td>-21.961937</td>\n",
       "      <td>-21.424906</td>\n",
       "      <td>-23.289913</td>\n",
       "      <td>-22.976345</td>\n",
       "      <td>-22.759392</td>\n",
       "      <td>-22.468496</td>\n",
       "      <td>-21.871695</td>\n",
       "      <td>-21.064466</td>\n",
       "      <td>...</td>\n",
       "      <td>-21.862230</td>\n",
       "      <td>-22.572437</td>\n",
       "      <td>-19.258432</td>\n",
       "      <td>-16.764656</td>\n",
       "      <td>-16.074083</td>\n",
       "      <td>-18.260128</td>\n",
       "      <td>-16.215405</td>\n",
       "      <td>-11.648987</td>\n",
       "      <td>-5.655075</td>\n",
       "      <td>male_fearful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1550</th>\n",
       "      <td>-55.532898</td>\n",
       "      <td>-55.532898</td>\n",
       "      <td>-55.532898</td>\n",
       "      <td>-55.532898</td>\n",
       "      <td>-55.749210</td>\n",
       "      <td>-55.568153</td>\n",
       "      <td>-54.801144</td>\n",
       "      <td>-54.083748</td>\n",
       "      <td>-54.007198</td>\n",
       "      <td>-53.115437</td>\n",
       "      <td>...</td>\n",
       "      <td>-34.338814</td>\n",
       "      <td>-35.185574</td>\n",
       "      <td>-36.500820</td>\n",
       "      <td>-36.251038</td>\n",
       "      <td>-36.079472</td>\n",
       "      <td>-38.075310</td>\n",
       "      <td>-38.165432</td>\n",
       "      <td>-25.012341</td>\n",
       "      <td>-16.912292</td>\n",
       "      <td>female_angry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>-70.400543</td>\n",
       "      <td>-70.400543</td>\n",
       "      <td>-70.400543</td>\n",
       "      <td>-70.400543</td>\n",
       "      <td>-69.432198</td>\n",
       "      <td>-67.132263</td>\n",
       "      <td>-66.026482</td>\n",
       "      <td>-65.041885</td>\n",
       "      <td>-64.778717</td>\n",
       "      <td>-65.204933</td>\n",
       "      <td>...</td>\n",
       "      <td>-61.743065</td>\n",
       "      <td>-58.785378</td>\n",
       "      <td>-58.077396</td>\n",
       "      <td>-59.684612</td>\n",
       "      <td>-61.275620</td>\n",
       "      <td>-60.361671</td>\n",
       "      <td>-57.225765</td>\n",
       "      <td>-58.686886</td>\n",
       "      <td>-63.324009</td>\n",
       "      <td>female_sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>610</th>\n",
       "      <td>-53.901146</td>\n",
       "      <td>-52.951214</td>\n",
       "      <td>-53.601170</td>\n",
       "      <td>-54.606205</td>\n",
       "      <td>-54.673542</td>\n",
       "      <td>-56.184101</td>\n",
       "      <td>-54.182045</td>\n",
       "      <td>-53.301315</td>\n",
       "      <td>-54.111423</td>\n",
       "      <td>-52.609173</td>\n",
       "      <td>...</td>\n",
       "      <td>-51.214806</td>\n",
       "      <td>-51.312046</td>\n",
       "      <td>-50.082497</td>\n",
       "      <td>-48.897903</td>\n",
       "      <td>-50.257988</td>\n",
       "      <td>-47.938293</td>\n",
       "      <td>-47.588028</td>\n",
       "      <td>-50.363785</td>\n",
       "      <td>-52.028522</td>\n",
       "      <td>male_angry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1376</th>\n",
       "      <td>-54.179581</td>\n",
       "      <td>-54.179581</td>\n",
       "      <td>-54.179581</td>\n",
       "      <td>-54.179581</td>\n",
       "      <td>-54.179581</td>\n",
       "      <td>-54.179581</td>\n",
       "      <td>-54.179581</td>\n",
       "      <td>-54.179581</td>\n",
       "      <td>-54.179581</td>\n",
       "      <td>-54.179581</td>\n",
       "      <td>...</td>\n",
       "      <td>-25.791344</td>\n",
       "      <td>-25.404562</td>\n",
       "      <td>-26.860264</td>\n",
       "      <td>-26.946703</td>\n",
       "      <td>-28.948444</td>\n",
       "      <td>-28.135712</td>\n",
       "      <td>-27.333925</td>\n",
       "      <td>-18.183626</td>\n",
       "      <td>-12.727003</td>\n",
       "      <td>male_sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1813</th>\n",
       "      <td>-32.338585</td>\n",
       "      <td>-32.157700</td>\n",
       "      <td>-32.110424</td>\n",
       "      <td>-32.799969</td>\n",
       "      <td>-32.624287</td>\n",
       "      <td>-32.610252</td>\n",
       "      <td>-32.409489</td>\n",
       "      <td>-33.093071</td>\n",
       "      <td>-32.054409</td>\n",
       "      <td>-31.906956</td>\n",
       "      <td>...</td>\n",
       "      <td>-20.083010</td>\n",
       "      <td>-19.547424</td>\n",
       "      <td>-18.529434</td>\n",
       "      <td>-17.725761</td>\n",
       "      <td>-18.311722</td>\n",
       "      <td>-19.524323</td>\n",
       "      <td>-19.709133</td>\n",
       "      <td>-18.645870</td>\n",
       "      <td>-17.298603</td>\n",
       "      <td>male_fearful</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 217 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0          1          2          3          4          5  \\\n",
       "1009 -59.283310 -59.283310 -59.283310 -59.283310 -59.283310 -59.283310   \n",
       "1101 -47.759697 -51.465084 -57.384605 -57.002815 -58.422222 -58.849869   \n",
       "1560 -44.249050 -47.272362 -48.930332 -49.305489 -48.988178 -48.861511   \n",
       "86   -58.587570 -56.556194 -56.574001 -53.165684 -52.330986 -55.095329   \n",
       "1969 -22.843060 -23.030672 -21.961937 -21.424906 -23.289913 -22.976345   \n",
       "1550 -55.532898 -55.532898 -55.532898 -55.532898 -55.749210 -55.568153   \n",
       "397  -70.400543 -70.400543 -70.400543 -70.400543 -69.432198 -67.132263   \n",
       "610  -53.901146 -52.951214 -53.601170 -54.606205 -54.673542 -56.184101   \n",
       "1376 -54.179581 -54.179581 -54.179581 -54.179581 -54.179581 -54.179581   \n",
       "1813 -32.338585 -32.157700 -32.110424 -32.799969 -32.624287 -32.610252   \n",
       "\n",
       "              6          7          8          9  ...        207        208  \\\n",
       "1009 -59.283310 -60.162197 -58.328056 -58.705463  ... -42.280170 -37.908653   \n",
       "1101 -57.612453 -56.452694 -53.982033 -51.257492  ... -37.422802 -37.302223   \n",
       "1560 -50.218369 -49.586102 -48.696918 -48.407051  ... -21.699717 -19.994274   \n",
       "86   -56.259312 -54.002625 -52.834641 -54.240810  ... -55.064278 -54.165936   \n",
       "1969 -22.759392 -22.468496 -21.871695 -21.064466  ... -21.862230 -22.572437   \n",
       "1550 -54.801144 -54.083748 -54.007198 -53.115437  ... -34.338814 -35.185574   \n",
       "397  -66.026482 -65.041885 -64.778717 -65.204933  ... -61.743065 -58.785378   \n",
       "610  -54.182045 -53.301315 -54.111423 -52.609173  ... -51.214806 -51.312046   \n",
       "1376 -54.179581 -54.179581 -54.179581 -54.179581  ... -25.791344 -25.404562   \n",
       "1813 -32.409489 -33.093071 -32.054409 -31.906956  ... -20.083010 -19.547424   \n",
       "\n",
       "            209        210        211        212        213        214  \\\n",
       "1009 -37.050064 -38.493828 -38.409847 -40.297588 -40.275467 -26.389082   \n",
       "1101 -38.155952 -40.875015 -41.355923 -40.960178 -41.197647 -31.651970   \n",
       "1560 -19.988308 -18.959019 -19.945028 -19.368382 -19.366259 -16.705296   \n",
       "86   -52.863503 -50.170200 -50.618374 -50.470371 -50.961624 -53.676678   \n",
       "1969 -19.258432 -16.764656 -16.074083 -18.260128 -16.215405 -11.648987   \n",
       "1550 -36.500820 -36.251038 -36.079472 -38.075310 -38.165432 -25.012341   \n",
       "397  -58.077396 -59.684612 -61.275620 -60.361671 -57.225765 -58.686886   \n",
       "610  -50.082497 -48.897903 -50.257988 -47.938293 -47.588028 -50.363785   \n",
       "1376 -26.860264 -26.946703 -28.948444 -28.135712 -27.333925 -18.183626   \n",
       "1813 -18.529434 -17.725761 -18.311722 -19.524323 -19.709133 -18.645870   \n",
       "\n",
       "            215         label  \n",
       "1009 -18.184639   female_calm  \n",
       "1101 -23.466846   female_calm  \n",
       "1560 -12.413276    male_angry  \n",
       "86   -54.448097     male_calm  \n",
       "1969  -5.655075  male_fearful  \n",
       "1550 -16.912292  female_angry  \n",
       "397  -63.324009    female_sad  \n",
       "610  -52.028522    male_angry  \n",
       "1376 -12.727003      male_sad  \n",
       "1813 -17.298603  male_fearful  \n",
       "\n",
       "[10 rows x 217 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "rnewdf = shuffle(newdf)\n",
    "rnewdf[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "rnewdf=rnewdf.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "newdf1 = np.random.rand(len(rnewdf)) < 0.8\n",
    "train = rnewdf[newdf1]\n",
    "test = rnewdf[~newdf1]\n",
    "# train1, test1 = train_test_split(rnewdf, test_size=0.2, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>207</th>\n",
       "      <th>208</th>\n",
       "      <th>209</th>\n",
       "      <th>210</th>\n",
       "      <th>211</th>\n",
       "      <th>212</th>\n",
       "      <th>213</th>\n",
       "      <th>214</th>\n",
       "      <th>215</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>458</th>\n",
       "      <td>-53.902451</td>\n",
       "      <td>-55.022114</td>\n",
       "      <td>-55.498245</td>\n",
       "      <td>-56.208687</td>\n",
       "      <td>-56.637962</td>\n",
       "      <td>-56.581551</td>\n",
       "      <td>-58.548874</td>\n",
       "      <td>-57.804241</td>\n",
       "      <td>-58.369339</td>\n",
       "      <td>-58.224583</td>\n",
       "      <td>...</td>\n",
       "      <td>-36.818577</td>\n",
       "      <td>-36.551991</td>\n",
       "      <td>-35.206345</td>\n",
       "      <td>-35.904900</td>\n",
       "      <td>-35.770969</td>\n",
       "      <td>-35.135532</td>\n",
       "      <td>-36.292080</td>\n",
       "      <td>-37.034821</td>\n",
       "      <td>-35.615425</td>\n",
       "      <td>male_sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2089</th>\n",
       "      <td>-26.575468</td>\n",
       "      <td>-26.038328</td>\n",
       "      <td>-27.055674</td>\n",
       "      <td>-27.812929</td>\n",
       "      <td>-25.842052</td>\n",
       "      <td>-25.779335</td>\n",
       "      <td>-25.921089</td>\n",
       "      <td>-24.171457</td>\n",
       "      <td>-25.842537</td>\n",
       "      <td>-25.202175</td>\n",
       "      <td>...</td>\n",
       "      <td>-27.084093</td>\n",
       "      <td>-26.680504</td>\n",
       "      <td>-25.796246</td>\n",
       "      <td>-26.386827</td>\n",
       "      <td>-27.404705</td>\n",
       "      <td>-27.704195</td>\n",
       "      <td>-26.367107</td>\n",
       "      <td>-26.645704</td>\n",
       "      <td>-27.550621</td>\n",
       "      <td>male_sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2104</th>\n",
       "      <td>-23.467129</td>\n",
       "      <td>-23.695148</td>\n",
       "      <td>-23.524519</td>\n",
       "      <td>-21.714996</td>\n",
       "      <td>-22.522959</td>\n",
       "      <td>-24.912157</td>\n",
       "      <td>-24.294329</td>\n",
       "      <td>-25.097324</td>\n",
       "      <td>-25.479826</td>\n",
       "      <td>-24.925625</td>\n",
       "      <td>...</td>\n",
       "      <td>-22.438875</td>\n",
       "      <td>-22.602425</td>\n",
       "      <td>-19.960636</td>\n",
       "      <td>-19.494825</td>\n",
       "      <td>-21.601143</td>\n",
       "      <td>-21.825146</td>\n",
       "      <td>-20.550596</td>\n",
       "      <td>-22.522329</td>\n",
       "      <td>-27.278515</td>\n",
       "      <td>male_sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1231</th>\n",
       "      <td>-55.181171</td>\n",
       "      <td>-55.181171</td>\n",
       "      <td>-55.181171</td>\n",
       "      <td>-55.181171</td>\n",
       "      <td>-55.181171</td>\n",
       "      <td>-55.181171</td>\n",
       "      <td>-55.181171</td>\n",
       "      <td>-55.181171</td>\n",
       "      <td>-55.181171</td>\n",
       "      <td>-55.181171</td>\n",
       "      <td>...</td>\n",
       "      <td>-43.770649</td>\n",
       "      <td>-42.729214</td>\n",
       "      <td>-43.623531</td>\n",
       "      <td>-43.907513</td>\n",
       "      <td>-45.279068</td>\n",
       "      <td>-44.657879</td>\n",
       "      <td>-42.104961</td>\n",
       "      <td>-33.019863</td>\n",
       "      <td>-25.573139</td>\n",
       "      <td>female_happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323</th>\n",
       "      <td>-48.638100</td>\n",
       "      <td>-48.638100</td>\n",
       "      <td>-48.638100</td>\n",
       "      <td>-48.638100</td>\n",
       "      <td>-48.638100</td>\n",
       "      <td>-48.638100</td>\n",
       "      <td>-48.638100</td>\n",
       "      <td>-48.076477</td>\n",
       "      <td>-47.975407</td>\n",
       "      <td>-47.726406</td>\n",
       "      <td>...</td>\n",
       "      <td>-34.005039</td>\n",
       "      <td>-30.143806</td>\n",
       "      <td>-28.844574</td>\n",
       "      <td>-28.764626</td>\n",
       "      <td>-29.490532</td>\n",
       "      <td>-29.049360</td>\n",
       "      <td>-27.577091</td>\n",
       "      <td>-21.164640</td>\n",
       "      <td>-14.801329</td>\n",
       "      <td>female_happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>794</th>\n",
       "      <td>-52.583378</td>\n",
       "      <td>-54.165749</td>\n",
       "      <td>-54.633442</td>\n",
       "      <td>-55.004471</td>\n",
       "      <td>-54.125935</td>\n",
       "      <td>-53.985195</td>\n",
       "      <td>-54.347694</td>\n",
       "      <td>-54.995201</td>\n",
       "      <td>-56.836052</td>\n",
       "      <td>-56.798771</td>\n",
       "      <td>...</td>\n",
       "      <td>-21.379808</td>\n",
       "      <td>-21.048082</td>\n",
       "      <td>-22.932808</td>\n",
       "      <td>-25.551434</td>\n",
       "      <td>-27.867006</td>\n",
       "      <td>-29.028154</td>\n",
       "      <td>-29.759718</td>\n",
       "      <td>-30.860910</td>\n",
       "      <td>-33.005081</td>\n",
       "      <td>male_fearful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1018</th>\n",
       "      <td>-56.978497</td>\n",
       "      <td>-59.122646</td>\n",
       "      <td>-63.025616</td>\n",
       "      <td>-64.729698</td>\n",
       "      <td>-64.729698</td>\n",
       "      <td>-64.729698</td>\n",
       "      <td>-62.736992</td>\n",
       "      <td>-60.936520</td>\n",
       "      <td>-61.440742</td>\n",
       "      <td>-61.334614</td>\n",
       "      <td>...</td>\n",
       "      <td>-34.817944</td>\n",
       "      <td>-36.015202</td>\n",
       "      <td>-39.292908</td>\n",
       "      <td>-39.562649</td>\n",
       "      <td>-39.105946</td>\n",
       "      <td>-39.705132</td>\n",
       "      <td>-38.961975</td>\n",
       "      <td>-34.584812</td>\n",
       "      <td>-30.217207</td>\n",
       "      <td>male_calm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>-56.697205</td>\n",
       "      <td>-53.862930</td>\n",
       "      <td>-52.127281</td>\n",
       "      <td>-49.995914</td>\n",
       "      <td>-49.359821</td>\n",
       "      <td>-47.952122</td>\n",
       "      <td>-48.595947</td>\n",
       "      <td>-50.867428</td>\n",
       "      <td>-51.143131</td>\n",
       "      <td>-48.616562</td>\n",
       "      <td>...</td>\n",
       "      <td>-27.294376</td>\n",
       "      <td>-27.235304</td>\n",
       "      <td>-27.843126</td>\n",
       "      <td>-29.807709</td>\n",
       "      <td>-30.955679</td>\n",
       "      <td>-30.760841</td>\n",
       "      <td>-29.833559</td>\n",
       "      <td>-29.842146</td>\n",
       "      <td>-29.075010</td>\n",
       "      <td>male_calm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1861</th>\n",
       "      <td>-58.073910</td>\n",
       "      <td>-55.233929</td>\n",
       "      <td>-54.841305</td>\n",
       "      <td>-55.691261</td>\n",
       "      <td>-56.029934</td>\n",
       "      <td>-55.878914</td>\n",
       "      <td>-56.141495</td>\n",
       "      <td>-58.162308</td>\n",
       "      <td>-55.570835</td>\n",
       "      <td>-54.631771</td>\n",
       "      <td>...</td>\n",
       "      <td>-35.360474</td>\n",
       "      <td>-37.285500</td>\n",
       "      <td>-36.379066</td>\n",
       "      <td>-36.728397</td>\n",
       "      <td>-37.930519</td>\n",
       "      <td>-37.505241</td>\n",
       "      <td>-37.762283</td>\n",
       "      <td>-39.493248</td>\n",
       "      <td>-39.911373</td>\n",
       "      <td>male_fearful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>-55.170769</td>\n",
       "      <td>-54.091942</td>\n",
       "      <td>-53.982773</td>\n",
       "      <td>-55.601238</td>\n",
       "      <td>-55.569981</td>\n",
       "      <td>-56.416954</td>\n",
       "      <td>-53.709347</td>\n",
       "      <td>-53.147556</td>\n",
       "      <td>-53.645657</td>\n",
       "      <td>-53.937557</td>\n",
       "      <td>...</td>\n",
       "      <td>-55.630547</td>\n",
       "      <td>-53.094551</td>\n",
       "      <td>-51.176643</td>\n",
       "      <td>-51.683449</td>\n",
       "      <td>-51.552547</td>\n",
       "      <td>-54.197231</td>\n",
       "      <td>-55.511971</td>\n",
       "      <td>-56.755936</td>\n",
       "      <td>-52.897682</td>\n",
       "      <td>male_happy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 217 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0          1          2          3          4          5  \\\n",
       "458  -53.902451 -55.022114 -55.498245 -56.208687 -56.637962 -56.581551   \n",
       "2089 -26.575468 -26.038328 -27.055674 -27.812929 -25.842052 -25.779335   \n",
       "2104 -23.467129 -23.695148 -23.524519 -21.714996 -22.522959 -24.912157   \n",
       "1231 -55.181171 -55.181171 -55.181171 -55.181171 -55.181171 -55.181171   \n",
       "323  -48.638100 -48.638100 -48.638100 -48.638100 -48.638100 -48.638100   \n",
       "794  -52.583378 -54.165749 -54.633442 -55.004471 -54.125935 -53.985195   \n",
       "1018 -56.978497 -59.122646 -63.025616 -64.729698 -64.729698 -64.729698   \n",
       "92   -56.697205 -53.862930 -52.127281 -49.995914 -49.359821 -47.952122   \n",
       "1861 -58.073910 -55.233929 -54.841305 -55.691261 -56.029934 -55.878914   \n",
       "202  -55.170769 -54.091942 -53.982773 -55.601238 -55.569981 -56.416954   \n",
       "\n",
       "              6          7          8          9  ...        207        208  \\\n",
       "458  -58.548874 -57.804241 -58.369339 -58.224583  ... -36.818577 -36.551991   \n",
       "2089 -25.921089 -24.171457 -25.842537 -25.202175  ... -27.084093 -26.680504   \n",
       "2104 -24.294329 -25.097324 -25.479826 -24.925625  ... -22.438875 -22.602425   \n",
       "1231 -55.181171 -55.181171 -55.181171 -55.181171  ... -43.770649 -42.729214   \n",
       "323  -48.638100 -48.076477 -47.975407 -47.726406  ... -34.005039 -30.143806   \n",
       "794  -54.347694 -54.995201 -56.836052 -56.798771  ... -21.379808 -21.048082   \n",
       "1018 -62.736992 -60.936520 -61.440742 -61.334614  ... -34.817944 -36.015202   \n",
       "92   -48.595947 -50.867428 -51.143131 -48.616562  ... -27.294376 -27.235304   \n",
       "1861 -56.141495 -58.162308 -55.570835 -54.631771  ... -35.360474 -37.285500   \n",
       "202  -53.709347 -53.147556 -53.645657 -53.937557  ... -55.630547 -53.094551   \n",
       "\n",
       "            209        210        211        212        213        214  \\\n",
       "458  -35.206345 -35.904900 -35.770969 -35.135532 -36.292080 -37.034821   \n",
       "2089 -25.796246 -26.386827 -27.404705 -27.704195 -26.367107 -26.645704   \n",
       "2104 -19.960636 -19.494825 -21.601143 -21.825146 -20.550596 -22.522329   \n",
       "1231 -43.623531 -43.907513 -45.279068 -44.657879 -42.104961 -33.019863   \n",
       "323  -28.844574 -28.764626 -29.490532 -29.049360 -27.577091 -21.164640   \n",
       "794  -22.932808 -25.551434 -27.867006 -29.028154 -29.759718 -30.860910   \n",
       "1018 -39.292908 -39.562649 -39.105946 -39.705132 -38.961975 -34.584812   \n",
       "92   -27.843126 -29.807709 -30.955679 -30.760841 -29.833559 -29.842146   \n",
       "1861 -36.379066 -36.728397 -37.930519 -37.505241 -37.762283 -39.493248   \n",
       "202  -51.176643 -51.683449 -51.552547 -54.197231 -55.511971 -56.755936   \n",
       "\n",
       "            215         label  \n",
       "458  -35.615425      male_sad  \n",
       "2089 -27.550621      male_sad  \n",
       "2104 -27.278515      male_sad  \n",
       "1231 -25.573139  female_happy  \n",
       "323  -14.801329  female_happy  \n",
       "794  -33.005081  male_fearful  \n",
       "1018 -30.217207     male_calm  \n",
       "92   -29.075010     male_calm  \n",
       "1861 -39.911373  male_fearful  \n",
       "202  -52.897682    male_happy  \n",
       "\n",
       "[10 rows x 217 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[250:260]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "trainfeatures = train.iloc[:, :-1]\n",
    "trainlabel = train.iloc[:, -1:]\n",
    "testfeatures = test.iloc[:, :-1]\n",
    "testlabel = test.iloc[:, -1:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\kalelt'has\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\utils\\validation.py:73: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import np_utils\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "X_train = np.array(trainfeatures)\n",
    "y_train = np.array(trainlabel)\n",
    "X_test = np.array(testfeatures)\n",
    "y_test = np.array(testlabel)\n",
    "\n",
    "lb = LabelEncoder()\n",
    "\n",
    "y_train = np_utils.to_categorical(lb.fit_transform(y_train))\n",
    "y_test = np_utils.to_categorical(lb.fit_transform(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1685, 216, 1)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_traincnn =np.expand_dims(X_train, axis=2)\n",
    "x_testcnn= np.expand_dims(X_test, axis=2)\n",
    "x_traincnn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv1D(256, 5,padding='same',\n",
    "                 input_shape=(216,1)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv1D(128, 5,padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(MaxPooling1D(pool_size=(8)))\n",
    "model.add(Conv1D(128, 5,padding='same',))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv1D(128, 5,padding='same',))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv1D(128, 5,padding='same',))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv1D(128, 5,padding='same',))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))\n",
    "opt = keras.optimizers.RMSprop(lr=0.00001, decay=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_12 (Conv1D)           (None, 216, 256)          1536      \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 216, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_13 (Conv1D)           (None, 216, 128)          163968    \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 216, 128)          0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 216, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 27, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_14 (Conv1D)           (None, 27, 128)           82048     \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 27, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_15 (Conv1D)           (None, 27, 128)           82048     \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 27, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_16 (Conv1D)           (None, 27, 128)           82048     \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 27, 128)           0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 27, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_17 (Conv1D)           (None, 27, 128)           82048     \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 27, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 3456)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                34570     \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 528,266\n",
      "Trainable params: 528,266\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer=opt,metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/700\n",
      "106/106 [==============================] - 9s 83ms/step - loss: 2.3623 - accuracy: 0.1139 - val_loss: 2.2646 - val_accuracy: 0.1701\n",
      "Epoch 2/700\n",
      "106/106 [==============================] - 8s 79ms/step - loss: 2.2818 - accuracy: 0.1353 - val_loss: 2.2348 - val_accuracy: 0.1862\n",
      "Epoch 3/700\n",
      "106/106 [==============================] - 8s 80ms/step - loss: 2.2454 - accuracy: 0.1608 - val_loss: 2.2186 - val_accuracy: 0.2069\n",
      "Epoch 4/700\n",
      "106/106 [==============================] - 9s 80ms/step - loss: 2.2157 - accuracy: 0.1703 - val_loss: 2.1709 - val_accuracy: 0.2299\n",
      "Epoch 5/700\n",
      "106/106 [==============================] - 8s 80ms/step - loss: 2.1746 - accuracy: 0.1929 - val_loss: 2.1457 - val_accuracy: 0.1908\n",
      "Epoch 6/700\n",
      "106/106 [==============================] - 8s 79ms/step - loss: 2.1315 - accuracy: 0.2101 - val_loss: 2.1030 - val_accuracy: 0.2092\n",
      "Epoch 7/700\n",
      "106/106 [==============================] - 8s 80ms/step - loss: 2.1044 - accuracy: 0.2178 - val_loss: 2.0473 - val_accuracy: 0.2506\n",
      "Epoch 8/700\n",
      "106/106 [==============================] - 9s 89ms/step - loss: 2.0529 - accuracy: 0.2421 - val_loss: 2.0098 - val_accuracy: 0.2483\n",
      "Epoch 9/700\n",
      "106/106 [==============================] - 10s 98ms/step - loss: 2.0206 - accuracy: 0.2273 - val_loss: 1.9551 - val_accuracy: 0.2644\n",
      "Epoch 10/700\n",
      "106/106 [==============================] - 11s 105ms/step - loss: 1.9640 - accuracy: 0.2528 - val_loss: 1.9231 - val_accuracy: 0.2828\n",
      "Epoch 11/700\n",
      "106/106 [==============================] - 11s 101ms/step - loss: 1.9205 - accuracy: 0.2605 - val_loss: 1.8673 - val_accuracy: 0.3149\n",
      "Epoch 12/700\n",
      "106/106 [==============================] - 10s 93ms/step - loss: 1.8748 - accuracy: 0.2872 - val_loss: 1.8494 - val_accuracy: 0.2736\n",
      "Epoch 13/700\n",
      "106/106 [==============================] - 9s 87ms/step - loss: 1.8383 - accuracy: 0.2712 - val_loss: 1.8077 - val_accuracy: 0.3057\n",
      "Epoch 14/700\n",
      "106/106 [==============================] - 8s 80ms/step - loss: 1.7945 - accuracy: 0.3039 - val_loss: 1.7726 - val_accuracy: 0.3057\n",
      "Epoch 15/700\n",
      "106/106 [==============================] - 8s 80ms/step - loss: 1.7669 - accuracy: 0.3050 - val_loss: 1.7463 - val_accuracy: 0.3356\n",
      "Epoch 16/700\n",
      "106/106 [==============================] - 9s 82ms/step - loss: 1.7494 - accuracy: 0.3027 - val_loss: 1.7117 - val_accuracy: 0.3356\n",
      "Epoch 17/700\n",
      "106/106 [==============================] - 9s 80ms/step - loss: 1.7046 - accuracy: 0.3264 - val_loss: 1.6966 - val_accuracy: 0.3172\n",
      "Epoch 18/700\n",
      "106/106 [==============================] - 9s 80ms/step - loss: 1.6796 - accuracy: 0.3513 - val_loss: 1.7130 - val_accuracy: 0.2966\n",
      "Epoch 19/700\n",
      "106/106 [==============================] - 8s 80ms/step - loss: 1.6743 - accuracy: 0.3312 - val_loss: 1.6635 - val_accuracy: 0.3195\n",
      "Epoch 20/700\n",
      "106/106 [==============================] - 9s 81ms/step - loss: 1.5581 - accuracy: 0.3697 - val_loss: 1.5658 - val_accuracy: 0.3471\n",
      "Epoch 28/700\n",
      "106/106 [==============================] - 8s 80ms/step - loss: 1.5485 - accuracy: 0.3810 - val_loss: 1.5707 - val_accuracy: 0.3724\n",
      "Epoch 29/700\n",
      "106/106 [==============================] - 8s 79ms/step - loss: 1.5452 - accuracy: 0.3751 - val_loss: 1.6011 - val_accuracy: 0.3586\n",
      "Epoch 30/700\n",
      "106/106 [==============================] - 8s 80ms/step - loss: 1.5312 - accuracy: 0.3822 - val_loss: 1.5834 - val_accuracy: 0.3402\n",
      "Epoch 31/700\n",
      "106/106 [==============================] - 8s 79ms/step - loss: 1.5312 - accuracy: 0.3822 - val_loss: 1.5635 - val_accuracy: 0.3747\n",
      "Epoch 32/700\n",
      "106/106 [==============================] - 8s 80ms/step - loss: 1.5345 - accuracy: 0.3887 - val_loss: 1.5854 - val_accuracy: 0.3494\n",
      "Epoch 33/700\n",
      "106/106 [==============================] - 8s 79ms/step - loss: 1.5230 - accuracy: 0.3875 - val_loss: 1.6272 - val_accuracy: 0.3310\n",
      "Epoch 34/700\n",
      "106/106 [==============================] - 9s 81ms/step - loss: 1.5075 - accuracy: 0.3917 - val_loss: 1.5161 - val_accuracy: 0.3862\n",
      "Epoch 35/700\n",
      "106/106 [==============================] - 9s 81ms/step - loss: 1.5014 - accuracy: 0.3929 - val_loss: 1.7294 - val_accuracy: 0.2943\n",
      "Epoch 36/700\n",
      "106/106 [==============================] - 9s 85ms/step - loss: 1.4932 - accuracy: 0.4059 - val_loss: 1.5133 - val_accuracy: 0.3839\n",
      "Epoch 37/700\n",
      "106/106 [==============================] - 8s 80ms/step - loss: 1.4759 - accuracy: 0.3964 - val_loss: 1.5082 - val_accuracy: 0.3908\n",
      "Epoch 38/700\n",
      "106/106 [==============================] - 8s 79ms/step - loss: 1.4674 - accuracy: 0.4000 - val_loss: 1.5741 - val_accuracy: 0.3540\n",
      "Epoch 39/700\n",
      "106/106 [==============================] - 8s 80ms/step - loss: 1.4827 - accuracy: 0.3941 - val_loss: 1.5152 - val_accuracy: 0.3839\n",
      "Epoch 40/700\n",
      "106/106 [==============================] - 8s 79ms/step - loss: 1.4498 - accuracy: 0.4053 - val_loss: 1.5311 - val_accuracy: 0.3678\n",
      "Epoch 41/700\n",
      "106/106 [==============================] - 8s 79ms/step - loss: 1.4430 - accuracy: 0.4172 - val_loss: 1.5407 - val_accuracy: 0.3793\n",
      "Epoch 42/700\n",
      "106/106 [==============================] - 8s 80ms/step - loss: 1.4261 - accuracy: 0.4309 - val_loss: 1.6729 - val_accuracy: 0.3149\n",
      "Epoch 43/700\n",
      "106/106 [==============================] - 9s 81ms/step - loss: 1.4505 - accuracy: 0.4267 - val_loss: 1.5391 - val_accuracy: 0.3678\n",
      "Epoch 44/700\n",
      "106/106 [==============================] - 8s 80ms/step - loss: 1.4463 - accuracy: 0.4136 - val_loss: 1.4775 - val_accuracy: 0.4276\n",
      "Epoch 45/700\n",
      "106/106 [==============================] - 8s 80ms/step - loss: 1.4332 - accuracy: 0.4172 - val_loss: 1.4979 - val_accuracy: 0.3977\n",
      "Epoch 46/700\n",
      "106/106 [==============================] - 8s 80ms/step - loss: 1.4305 - accuracy: 0.4154 - val_loss: 1.5020 - val_accuracy: 0.4000\n",
      "Epoch 47/700\n",
      "106/106 [==============================] - 8s 79ms/step - loss: 1.3610 - accuracy: 0.4611 - val_loss: 1.4576 - val_accuracy: 0.4069\n",
      "Epoch 58/700\n",
      "106/106 [==============================] - 8s 79ms/step - loss: 1.3664 - accuracy: 0.4475 - val_loss: 1.4746 - val_accuracy: 0.4023\n",
      "Epoch 59/700\n",
      "106/106 [==============================] - 8s 79ms/step - loss: 1.3534 - accuracy: 0.4576 - val_loss: 1.6793 - val_accuracy: 0.3379\n",
      "Epoch 60/700\n",
      "106/106 [==============================] - 8s 79ms/step - loss: 1.3562 - accuracy: 0.4570 - val_loss: 1.5049 - val_accuracy: 0.3954\n",
      "Epoch 61/700\n",
      "106/106 [==============================] - 9s 82ms/step - loss: 1.3490 - accuracy: 0.4605 - val_loss: 1.4958 - val_accuracy: 0.3908\n",
      "Epoch 62/700\n",
      "106/106 [==============================] - 10s 94ms/step - loss: 1.3340 - accuracy: 0.4659 - val_loss: 1.4687 - val_accuracy: 0.4115\n",
      "Epoch 63/700\n",
      "106/106 [==============================] - 9s 84ms/step - loss: 1.3400 - accuracy: 0.4641 - val_loss: 1.4824 - val_accuracy: 0.3977\n",
      "Epoch 64/700\n",
      "106/106 [==============================] - 9s 82ms/step - loss: 1.3343 - accuracy: 0.4706 - val_loss: 1.4758 - val_accuracy: 0.4000\n",
      "Epoch 65/700\n",
      "106/106 [==============================] - 8s 80ms/step - loss: 1.3226 - accuracy: 0.4659 - val_loss: 1.4661 - val_accuracy: 0.3977\n",
      "Epoch 66/700\n",
      "106/106 [==============================] - 8s 79ms/step - loss: 1.3230 - accuracy: 0.4742 - val_loss: 1.4512 - val_accuracy: 0.4092\n",
      "Epoch 67/700\n",
      "106/106 [==============================] - 9s 82ms/step - loss: 1.3143 - accuracy: 0.4724 - val_loss: 1.4881 - val_accuracy: 0.4023\n",
      "Epoch 68/700\n",
      "106/106 [==============================] - 9s 82ms/step - loss: 1.3183 - accuracy: 0.4688 - val_loss: 1.4879 - val_accuracy: 0.3839\n",
      "Epoch 69/700\n",
      "106/106 [==============================] - 8s 79ms/step - loss: 1.3281 - accuracy: 0.4760 - val_loss: 1.4596 - val_accuracy: 0.3977\n",
      "Epoch 70/700\n",
      "106/106 [==============================] - 9s 85ms/step - loss: 1.3128 - accuracy: 0.4724 - val_loss: 1.5544 - val_accuracy: 0.3862\n",
      "Epoch 71/700\n",
      "106/106 [==============================] - 8s 79ms/step - loss: 1.3136 - accuracy: 0.4795 - val_loss: 1.4506 - val_accuracy: 0.4207\n",
      "Epoch 72/700\n",
      "106/106 [==============================] - 9s 82ms/step - loss: 1.2923 - accuracy: 0.4920 - val_loss: 1.4356 - val_accuracy: 0.4276\n",
      "Epoch 73/700\n",
      "106/106 [==============================] - 8s 79ms/step - loss: 1.2947 - accuracy: 0.4742 - val_loss: 1.4622 - val_accuracy: 0.3931\n",
      "Epoch 74/700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106/106 [==============================] - 8s 79ms/step - loss: 1.2861 - accuracy: 0.4872 - val_loss: 1.5754 - val_accuracy: 0.3678\n",
      "Epoch 75/700\n",
      "106/106 [==============================] - 8s 80ms/step - loss: 1.2923 - accuracy: 0.4795 - val_loss: 1.4613 - val_accuracy: 0.4299\n",
      "Epoch 76/700\n",
      "106/106 [==============================] - 9s 84ms/step - loss: 1.2432 - accuracy: 0.5163 - val_loss: 1.4456 - val_accuracy: 0.4115\n",
      "Epoch 90/700\n",
      "106/106 [==============================] - 9s 85ms/step - loss: 1.2296 - accuracy: 0.5157 - val_loss: 1.4095 - val_accuracy: 0.4391\n",
      "Epoch 91/700\n",
      "106/106 [==============================] - 9s 83ms/step - loss: 1.2136 - accuracy: 0.5175 - val_loss: 1.5144 - val_accuracy: 0.4000\n",
      "Epoch 92/700\n",
      "106/106 [==============================] - 8s 79ms/step - loss: 1.2264 - accuracy: 0.5276 - val_loss: 1.4184 - val_accuracy: 0.4391\n",
      "Epoch 93/700\n",
      "106/106 [==============================] - 9s 82ms/step - loss: 1.2387 - accuracy: 0.5056 - val_loss: 1.8545 - val_accuracy: 0.2828\n",
      "Epoch 94/700\n",
      "106/106 [==============================] - 9s 85ms/step - loss: 1.2130 - accuracy: 0.5246 - val_loss: 1.4025 - val_accuracy: 0.4322\n",
      "Epoch 95/700\n",
      "106/106 [==============================] - 8s 79ms/step - loss: 1.2274 - accuracy: 0.5157 - val_loss: 1.3964 - val_accuracy: 0.4437\n",
      "Epoch 96/700\n",
      "106/106 [==============================] - 8s 77ms/step - loss: 1.2197 - accuracy: 0.5217 - val_loss: 1.5267 - val_accuracy: 0.4023\n",
      "Epoch 97/700\n",
      "106/106 [==============================] - 8s 78ms/step - loss: 1.2044 - accuracy: 0.5430 - val_loss: 1.4274 - val_accuracy: 0.4529\n",
      "Epoch 98/700\n",
      "106/106 [==============================] - 8s 76ms/step - loss: 1.2032 - accuracy: 0.5288 - val_loss: 1.4130 - val_accuracy: 0.4276\n",
      "Epoch 99/700\n",
      "106/106 [==============================] - 8s 77ms/step - loss: 1.2148 - accuracy: 0.5217 - val_loss: 1.4020 - val_accuracy: 0.4483\n",
      "Epoch 100/700\n",
      "106/106 [==============================] - 9s 81ms/step - loss: 1.1963 - accuracy: 0.5424 - val_loss: 1.4049 - val_accuracy: 0.4276\n",
      "Epoch 101/700\n",
      "106/106 [==============================] - 9s 82ms/step - loss: 1.2078 - accuracy: 0.5128 - val_loss: 1.4072 - val_accuracy: 0.4391\n",
      "Epoch 102/700\n",
      "106/106 [==============================] - 8s 78ms/step - loss: 1.1933 - accuracy: 0.5157 - val_loss: 1.4698 - val_accuracy: 0.4161\n",
      "Epoch 103/700\n",
      "106/106 [==============================] - 9s 82ms/step - loss: 1.1882 - accuracy: 0.5490 - val_loss: 1.3994 - val_accuracy: 0.4414\n",
      "Epoch 104/700\n",
      "106/106 [==============================] - 8s 79ms/step - loss: 1.1932 - accuracy: 0.5359 - val_loss: 1.4829 - val_accuracy: 0.3931\n",
      "Epoch 105/700\n",
      "106/106 [==============================] - 9s 84ms/step - loss: 1.1276 - accuracy: 0.5638 - val_loss: 1.4329 - val_accuracy: 0.4161\n",
      "Epoch 116/700\n",
      "106/106 [==============================] - 9s 87ms/step - loss: 1.1427 - accuracy: 0.5460 - val_loss: 1.4273 - val_accuracy: 0.4391\n",
      "Epoch 117/700\n",
      "106/106 [==============================] - 9s 81ms/step - loss: 1.1388 - accuracy: 0.5537 - val_loss: 1.4121 - val_accuracy: 0.4575\n",
      "Epoch 118/700\n",
      "106/106 [==============================] - 8s 78ms/step - loss: 1.1362 - accuracy: 0.5501 - val_loss: 1.3860 - val_accuracy: 0.4391\n",
      "Epoch 119/700\n",
      "106/106 [==============================] - 8s 80ms/step - loss: 1.1393 - accuracy: 0.5573 - val_loss: 1.4309 - val_accuracy: 0.4391\n",
      "Epoch 120/700\n",
      "106/106 [==============================] - 8s 79ms/step - loss: 1.1332 - accuracy: 0.5525 - val_loss: 1.4207 - val_accuracy: 0.4368\n",
      "Epoch 121/700\n",
      "106/106 [==============================] - 8s 78ms/step - loss: 1.1321 - accuracy: 0.5662 - val_loss: 1.4224 - val_accuracy: 0.4322\n",
      "Epoch 122/700\n",
      "106/106 [==============================] - 8s 78ms/step - loss: 1.1446 - accuracy: 0.5466 - val_loss: 1.4281 - val_accuracy: 0.4184\n",
      "Epoch 123/700\n",
      "106/106 [==============================] - 8s 78ms/step - loss: 1.1132 - accuracy: 0.5769 - val_loss: 1.4343 - val_accuracy: 0.4161\n",
      "Epoch 124/700\n",
      "106/106 [==============================] - 8s 79ms/step - loss: 1.1276 - accuracy: 0.5596 - val_loss: 1.4063 - val_accuracy: 0.4345\n",
      "Epoch 125/700\n",
      "106/106 [==============================] - 8s 80ms/step - loss: 1.1227 - accuracy: 0.5680 - val_loss: 1.4363 - val_accuracy: 0.4184\n",
      "Epoch 126/700\n",
      "106/106 [==============================] - 8s 77ms/step - loss: 1.1139 - accuracy: 0.5596 - val_loss: 1.3846 - val_accuracy: 0.4506\n",
      "Epoch 127/700\n",
      "106/106 [==============================] - 9s 83ms/step - loss: 1.1231 - accuracy: 0.5680 - val_loss: 1.4825 - val_accuracy: 0.4115\n",
      "Epoch 128/700\n",
      "106/106 [==============================] - 9s 81ms/step - loss: 1.1074 - accuracy: 0.5573 - val_loss: 1.3902 - val_accuracy: 0.4483\n",
      "Epoch 129/700\n",
      "106/106 [==============================] - 8s 80ms/step - loss: 1.1153 - accuracy: 0.5668 - val_loss: 1.4582 - val_accuracy: 0.4322\n",
      "Epoch 130/700\n",
      "106/106 [==============================] - 9s 80ms/step - loss: 1.1047 - accuracy: 0.5822 - val_loss: 1.4047 - val_accuracy: 0.4368\n",
      "Epoch 131/700\n",
      "106/106 [==============================] - 8s 78ms/step - loss: 1.1055 - accuracy: 0.5644 - val_loss: 1.3876 - val_accuracy: 0.4483\n",
      "Epoch 132/700\n",
      "106/106 [==============================] - 8s 79ms/step - loss: 1.1010 - accuracy: 0.5757 - val_loss: 1.4558 - val_accuracy: 0.4368\n",
      "Epoch 133/700\n",
      "106/106 [==============================] - 9s 80ms/step - loss: 1.0768 - accuracy: 0.5881 - val_loss: 1.4114 - val_accuracy: 0.4391\n",
      "Epoch 140/700\n",
      "106/106 [==============================] - 9s 81ms/step - loss: 1.0695 - accuracy: 0.5834 - val_loss: 1.3866 - val_accuracy: 0.4483\n",
      "Epoch 141/700\n",
      "106/106 [==============================] - 9s 81ms/step - loss: 1.0624 - accuracy: 0.5923 - val_loss: 1.4332 - val_accuracy: 0.4460\n",
      "Epoch 142/700\n",
      "106/106 [==============================] - 8s 80ms/step - loss: 1.0605 - accuracy: 0.5911 - val_loss: 1.4176 - val_accuracy: 0.4391\n",
      "Epoch 143/700\n",
      "106/106 [==============================] - 8s 79ms/step - loss: 1.0659 - accuracy: 0.6000 - val_loss: 1.3923 - val_accuracy: 0.4437\n",
      "Epoch 144/700\n",
      "106/106 [==============================] - 8s 79ms/step - loss: 1.0666 - accuracy: 0.5905 - val_loss: 1.5112 - val_accuracy: 0.4161\n",
      "Epoch 145/700\n",
      "106/106 [==============================] - 9s 81ms/step - loss: 1.0486 - accuracy: 0.5976 - val_loss: 1.3951 - val_accuracy: 0.4414\n",
      "Epoch 146/700\n",
      "106/106 [==============================] - 9s 81ms/step - loss: 1.0551 - accuracy: 0.5828 - val_loss: 1.4339 - val_accuracy: 0.4322\n",
      "Epoch 147/700\n",
      "106/106 [==============================] - 9s 86ms/step - loss: 1.0494 - accuracy: 0.5881 - val_loss: 1.3928 - val_accuracy: 0.4529\n",
      "Epoch 148/700\n",
      "106/106 [==============================] - 8s 78ms/step - loss: 1.0447 - accuracy: 0.5953 - val_loss: 1.4781 - val_accuracy: 0.4207\n",
      "Epoch 149/700\n",
      "106/106 [==============================] - 8s 78ms/step - loss: 1.0520 - accuracy: 0.5816 - val_loss: 1.3985 - val_accuracy: 0.4460\n",
      "Epoch 150/700\n",
      "106/106 [==============================] - 8s 78ms/step - loss: 1.0514 - accuracy: 0.5958 - val_loss: 1.3875 - val_accuracy: 0.4598\n",
      "Epoch 151/700\n",
      "106/106 [==============================] - 9s 84ms/step - loss: 1.0358 - accuracy: 0.6125 - val_loss: 1.3962 - val_accuracy: 0.4575\n",
      "Epoch 152/700\n",
      "106/106 [==============================] - 8s 78ms/step - loss: 1.0402 - accuracy: 0.5988 - val_loss: 1.4564 - val_accuracy: 0.4368\n",
      "Epoch 153/700\n",
      "106/106 [==============================] - 8s 77ms/step - loss: 1.0360 - accuracy: 0.5941 - val_loss: 1.3948 - val_accuracy: 0.4552\n",
      "Epoch 154/700\n",
      "106/106 [==============================] - 8s 77ms/step - loss: 1.0204 - accuracy: 0.6024 - val_loss: 1.4899 - val_accuracy: 0.4368\n",
      "Epoch 155/700\n",
      "106/106 [==============================] - 8s 78ms/step - loss: 1.0295 - accuracy: 0.6042 - val_loss: 1.4085 - val_accuracy: 0.4506\n",
      "Epoch 156/700\n",
      "106/106 [==============================] - 8s 76ms/step - loss: 1.0304 - accuracy: 0.6065 - val_loss: 1.4184 - val_accuracy: 0.4414\n",
      "Epoch 157/700\n",
      "106/106 [==============================] - 8s 78ms/step - loss: 1.0200 - accuracy: 0.6113 - val_loss: 1.4410 - val_accuracy: 0.4299\n",
      "Epoch 158/700\n",
      "106/106 [==============================] - 8s 76ms/step - loss: 1.0157 - accuracy: 0.6030 - val_loss: 1.4122 - val_accuracy: 0.4414\n",
      "Epoch 159/700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106/106 [==============================] - 8s 76ms/step - loss: 1.0141 - accuracy: 0.6042 - val_loss: 1.4639 - val_accuracy: 0.4253\n",
      "Epoch 160/700\n",
      "106/106 [==============================] - 8s 78ms/step - loss: 1.0105 - accuracy: 0.6030 - val_loss: 1.5133 - val_accuracy: 0.4138\n",
      "Epoch 161/700\n",
      "106/106 [==============================] - 8s 78ms/step - loss: 0.9862 - accuracy: 0.6184 - val_loss: 1.4487 - val_accuracy: 0.4299\n",
      "Epoch 170/700\n",
      "106/106 [==============================] - 8s 78ms/step - loss: 0.9757 - accuracy: 0.6237 - val_loss: 1.4766 - val_accuracy: 0.4299\n",
      "Epoch 171/700\n",
      "106/106 [==============================] - 9s 80ms/step - loss: 0.9949 - accuracy: 0.6166 - val_loss: 1.3985 - val_accuracy: 0.4529\n",
      "Epoch 172/700\n",
      "106/106 [==============================] - 8s 78ms/step - loss: 0.9700 - accuracy: 0.6190 - val_loss: 1.3823 - val_accuracy: 0.4460\n",
      "Epoch 173/700\n",
      "106/106 [==============================] - 8s 79ms/step - loss: 0.9704 - accuracy: 0.6243 - val_loss: 1.3988 - val_accuracy: 0.4460\n",
      "Epoch 174/700\n",
      "106/106 [==============================] - 8s 78ms/step - loss: 0.9697 - accuracy: 0.6297 - val_loss: 1.3980 - val_accuracy: 0.4598\n",
      "Epoch 175/700\n",
      "106/106 [==============================] - 8s 78ms/step - loss: 0.9735 - accuracy: 0.6362 - val_loss: 1.4350 - val_accuracy: 0.4460\n",
      "Epoch 176/700\n",
      "106/106 [==============================] - 9s 80ms/step - loss: 0.9592 - accuracy: 0.6469 - val_loss: 1.4143 - val_accuracy: 0.4460\n",
      "Epoch 177/700\n",
      "106/106 [==============================] - 9s 88ms/step - loss: 0.9716 - accuracy: 0.6261 - val_loss: 1.3785 - val_accuracy: 0.4460\n",
      "Epoch 178/700\n",
      "106/106 [==============================] - 9s 88ms/step - loss: 0.9650 - accuracy: 0.6332 - val_loss: 1.4791 - val_accuracy: 0.4276\n",
      "Epoch 179/700\n",
      "106/106 [==============================] - 8s 79ms/step - loss: 0.9485 - accuracy: 0.6457 - val_loss: 1.4638 - val_accuracy: 0.4483\n",
      "Epoch 180/700\n",
      "106/106 [==============================] - 8s 79ms/step - loss: 0.9504 - accuracy: 0.6261 - val_loss: 1.4072 - val_accuracy: 0.4552\n",
      "Epoch 181/700\n",
      "106/106 [==============================] - 8s 77ms/step - loss: 0.9495 - accuracy: 0.6368 - val_loss: 1.4101 - val_accuracy: 0.4529\n",
      "Epoch 182/700\n",
      "106/106 [==============================] - 8s 79ms/step - loss: 0.9486 - accuracy: 0.6356 - val_loss: 1.5496 - val_accuracy: 0.4046\n",
      "Epoch 183/700\n",
      "106/106 [==============================] - 9s 88ms/step - loss: 0.9419 - accuracy: 0.6593 - val_loss: 1.4056 - val_accuracy: 0.4529\n",
      "Epoch 184/700\n",
      "106/106 [==============================] - 9s 81ms/step - loss: 0.9273 - accuracy: 0.6463 - val_loss: 1.5574 - val_accuracy: 0.4161\n",
      "Epoch 185/700\n",
      "106/106 [==============================] - 9s 86ms/step - loss: 0.9390 - accuracy: 0.6297 - val_loss: 1.4061 - val_accuracy: 0.4552\n",
      "Epoch 186/700\n",
      "106/106 [==============================] - 9s 84ms/step - loss: 0.9545 - accuracy: 0.6350 - val_loss: 1.5031 - val_accuracy: 0.4253\n",
      "Epoch 187/700\n",
      "106/106 [==============================] - 9s 84ms/step - loss: 0.9293 - accuracy: 0.6457 - val_loss: 1.4082 - val_accuracy: 0.4506\n",
      "Epoch 188/700\n",
      "106/106 [==============================] - 9s 81ms/step - loss: 0.9351 - accuracy: 0.6404 - val_loss: 1.4323 - val_accuracy: 0.4598\n",
      "Epoch 189/700\n",
      "106/106 [==============================] - 8s 78ms/step - loss: 0.9297 - accuracy: 0.6421 - val_loss: 1.4242 - val_accuracy: 0.4483\n",
      "Epoch 190/700\n",
      "106/106 [==============================] - 8s 78ms/step - loss: 0.9281 - accuracy: 0.6409 - val_loss: 1.4231 - val_accuracy: 0.4575\n",
      "Epoch 191/700\n",
      "106/106 [==============================] - 8s 79ms/step - loss: 0.8887 - accuracy: 0.6623 - val_loss: 1.4365 - val_accuracy: 0.4575\n",
      "Epoch 208/700\n",
      "106/106 [==============================] - 8s 79ms/step - loss: 0.8580 - accuracy: 0.6807 - val_loss: 1.4006 - val_accuracy: 0.4414\n",
      "Epoch 209/700\n",
      "106/106 [==============================] - 9s 89ms/step - loss: 0.8867 - accuracy: 0.6623 - val_loss: 1.4178 - val_accuracy: 0.4621\n",
      "Epoch 210/700\n",
      "106/106 [==============================] - 8s 79ms/step - loss: 0.8799 - accuracy: 0.6653 - val_loss: 1.4031 - val_accuracy: 0.4483\n",
      "Epoch 211/700\n",
      "106/106 [==============================] - 9s 81ms/step - loss: 0.8599 - accuracy: 0.6801 - val_loss: 1.4770 - val_accuracy: 0.4437\n",
      "Epoch 212/700\n",
      "106/106 [==============================] - 9s 89ms/step - loss: 0.8784 - accuracy: 0.6659 - val_loss: 1.4401 - val_accuracy: 0.4667\n",
      "Epoch 213/700\n",
      "106/106 [==============================] - 9s 89ms/step - loss: 0.8656 - accuracy: 0.6724 - val_loss: 1.4346 - val_accuracy: 0.4483\n",
      "Epoch 214/700\n",
      "106/106 [==============================] - 9s 88ms/step - loss: 0.8621 - accuracy: 0.6748 - val_loss: 1.4482 - val_accuracy: 0.4460\n",
      "Epoch 215/700\n",
      "106/106 [==============================] - 10s 94ms/step - loss: 0.8475 - accuracy: 0.6760 - val_loss: 1.4245 - val_accuracy: 0.4276\n",
      "Epoch 216/700\n",
      "106/106 [==============================] - 10s 93ms/step - loss: 0.8658 - accuracy: 0.6659 - val_loss: 1.4210 - val_accuracy: 0.4598\n",
      "Epoch 217/700\n",
      "106/106 [==============================] - 9s 86ms/step - loss: 0.8638 - accuracy: 0.6677 - val_loss: 1.4564 - val_accuracy: 0.4667\n",
      "Epoch 218/700\n",
      "106/106 [==============================] - 8s 80ms/step - loss: 0.8520 - accuracy: 0.6718 - val_loss: 1.4733 - val_accuracy: 0.4460\n",
      "Epoch 219/700\n",
      "106/106 [==============================] - 8s 79ms/step - loss: 0.8403 - accuracy: 0.6795 - val_loss: 1.4494 - val_accuracy: 0.4598\n",
      "Epoch 220/700\n",
      "106/106 [==============================] - 10s 94ms/step - loss: 0.8519 - accuracy: 0.6843 - val_loss: 1.4048 - val_accuracy: 0.4552\n",
      "Epoch 221/700\n",
      "106/106 [==============================] - 10s 94ms/step - loss: 0.8337 - accuracy: 0.6718 - val_loss: 1.4176 - val_accuracy: 0.4621\n",
      "Epoch 222/700\n",
      "106/106 [==============================] - 9s 88ms/step - loss: 0.8528 - accuracy: 0.6777 - val_loss: 1.4204 - val_accuracy: 0.4437\n",
      "Epoch 223/700\n",
      "106/106 [==============================] - 9s 87ms/step - loss: 0.8484 - accuracy: 0.6659 - val_loss: 1.4270 - val_accuracy: 0.4345\n",
      "Epoch 224/700\n",
      "106/106 [==============================] - 8s 78ms/step - loss: 0.8379 - accuracy: 0.6819 - val_loss: 1.4479 - val_accuracy: 0.4483\n",
      "Epoch 225/700\n",
      "106/106 [==============================] - 8s 78ms/step - loss: 0.8401 - accuracy: 0.6819 - val_loss: 1.4255 - val_accuracy: 0.4552\n",
      "Epoch 226/700\n",
      "106/106 [==============================] - 8s 78ms/step - loss: 0.8268 - accuracy: 0.6831 - val_loss: 1.4140 - val_accuracy: 0.4483\n",
      "Epoch 227/700\n",
      "106/106 [==============================] - 8s 77ms/step - loss: 0.8291 - accuracy: 0.6902 - val_loss: 1.4352 - val_accuracy: 0.4483\n",
      "Epoch 228/700\n",
      "106/106 [==============================] - 9s 80ms/step - loss: 0.8422 - accuracy: 0.6843 - val_loss: 1.4750 - val_accuracy: 0.4506\n",
      "Epoch 229/700\n",
      "106/106 [==============================] - 9s 84ms/step - loss: 0.8276 - accuracy: 0.6932 - val_loss: 1.4418 - val_accuracy: 0.4506\n",
      "Epoch 230/700\n",
      "106/106 [==============================] - 10s 94ms/step - loss: 0.8268 - accuracy: 0.6777 - val_loss: 1.4289 - val_accuracy: 0.4621\n",
      "Epoch 231/700\n",
      "106/106 [==============================] - 9s 85ms/step - loss: 0.7829 - accuracy: 0.7116 - val_loss: 1.5072 - val_accuracy: 0.4483\n",
      "Epoch 243/700\n",
      "106/106 [==============================] - 9s 87ms/step - loss: 0.7938 - accuracy: 0.7122 - val_loss: 1.4495 - val_accuracy: 0.4529\n",
      "Epoch 244/700\n",
      "106/106 [==============================] - 9s 83ms/step - loss: 0.7027 - accuracy: 0.7418 - val_loss: 1.5500 - val_accuracy: 0.4391\n",
      "Epoch 273/700\n",
      "106/106 [==============================] - 9s 84ms/step - loss: 0.7195 - accuracy: 0.7466 - val_loss: 1.5539 - val_accuracy: 0.4368\n",
      "Epoch 274/700\n",
      "106/106 [==============================] - 9s 84ms/step - loss: 0.7247 - accuracy: 0.7430 - val_loss: 1.4960 - val_accuracy: 0.4483\n",
      "Epoch 275/700\n",
      "106/106 [==============================] - 9s 82ms/step - loss: 0.7036 - accuracy: 0.7383 - val_loss: 1.5077 - val_accuracy: 0.4345\n",
      "Epoch 276/700\n",
      "106/106 [==============================] - 10s 95ms/step - loss: 0.7033 - accuracy: 0.7377 - val_loss: 1.4885 - val_accuracy: 0.4644\n",
      "Epoch 277/700\n",
      "106/106 [==============================] - 9s 88ms/step - loss: 0.7015 - accuracy: 0.7323 - val_loss: 1.5319 - val_accuracy: 0.4575\n",
      "Epoch 278/700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106/106 [==============================] - 9s 84ms/step - loss: 0.7014 - accuracy: 0.7442 - val_loss: 1.4691 - val_accuracy: 0.4552\n",
      "Epoch 279/700\n",
      "106/106 [==============================] - 8s 80ms/step - loss: 0.7038 - accuracy: 0.7501 - val_loss: 1.5639 - val_accuracy: 0.4437\n",
      "Epoch 280/700\n",
      "106/106 [==============================] - 9s 89ms/step - loss: 0.6923 - accuracy: 0.7549 - val_loss: 1.6085 - val_accuracy: 0.4552\n",
      "Epoch 281/700\n",
      "106/106 [==============================] - 10s 93ms/step - loss: 0.7007 - accuracy: 0.7323 - val_loss: 1.5337 - val_accuracy: 0.4414\n",
      "Epoch 282/700\n",
      "106/106 [==============================] - 10s 95ms/step - loss: 0.7025 - accuracy: 0.7543 - val_loss: 1.4971 - val_accuracy: 0.4529\n",
      "Epoch 283/700\n",
      "106/106 [==============================] - 10s 91ms/step - loss: 0.6809 - accuracy: 0.7579 - val_loss: 1.6294 - val_accuracy: 0.4391\n",
      "Epoch 284/700\n",
      "106/106 [==============================] - 9s 89ms/step - loss: 0.6930 - accuracy: 0.7365 - val_loss: 1.5009 - val_accuracy: 0.4598\n",
      "Epoch 285/700\n",
      "106/106 [==============================] - 10s 90ms/step - loss: 0.6852 - accuracy: 0.7454 - val_loss: 1.4825 - val_accuracy: 0.4552\n",
      "Epoch 286/700\n",
      "106/106 [==============================] - 10s 93ms/step - loss: 0.6798 - accuracy: 0.7472 - val_loss: 1.5269 - val_accuracy: 0.4644\n",
      "Epoch 287/700\n",
      "106/106 [==============================] - 10s 93ms/step - loss: 0.6755 - accuracy: 0.7561 - val_loss: 1.4853 - val_accuracy: 0.4437\n",
      "Epoch 288/700\n",
      "106/106 [==============================] - 9s 84ms/step - loss: 0.6572 - accuracy: 0.7650 - val_loss: 1.5121 - val_accuracy: 0.4552\n",
      "Epoch 296/700\n",
      "106/106 [==============================] - 9s 84ms/step - loss: 0.6519 - accuracy: 0.7501 - val_loss: 1.5250 - val_accuracy: 0.4621\n",
      "Epoch 297/700\n",
      "106/106 [==============================] - 9s 82ms/step - loss: 0.6639 - accuracy: 0.7454 - val_loss: 1.5346 - val_accuracy: 0.4598\n",
      "Epoch 298/700\n",
      "106/106 [==============================] - 9s 81ms/step - loss: 0.6574 - accuracy: 0.7490 - val_loss: 1.5421 - val_accuracy: 0.4529\n",
      "Epoch 299/700\n",
      "106/106 [==============================] - 9s 84ms/step - loss: 0.6604 - accuracy: 0.7638 - val_loss: 1.5043 - val_accuracy: 0.4529\n",
      "Epoch 300/700\n",
      "106/106 [==============================] - 9s 84ms/step - loss: 0.6415 - accuracy: 0.7656 - val_loss: 1.5353 - val_accuracy: 0.4575\n",
      "Epoch 301/700\n",
      "106/106 [==============================] - 9s 83ms/step - loss: 0.6335 - accuracy: 0.7721 - val_loss: 1.5036 - val_accuracy: 0.4506\n",
      "Epoch 302/700\n",
      "106/106 [==============================] - 9s 83ms/step - loss: 0.6392 - accuracy: 0.7644 - val_loss: 1.5126 - val_accuracy: 0.4460\n",
      "Epoch 303/700\n",
      "106/106 [==============================] - 9s 84ms/step - loss: 0.6290 - accuracy: 0.7531 - val_loss: 1.6218 - val_accuracy: 0.4368\n",
      "Epoch 304/700\n",
      "106/106 [==============================] - 9s 88ms/step - loss: 0.6353 - accuracy: 0.7668 - val_loss: 1.5130 - val_accuracy: 0.4529\n",
      "Epoch 305/700\n",
      "106/106 [==============================] - 9s 82ms/step - loss: 0.6252 - accuracy: 0.7733 - val_loss: 1.5103 - val_accuracy: 0.4575\n",
      "Epoch 306/700\n",
      "106/106 [==============================] - 9s 86ms/step - loss: 0.6235 - accuracy: 0.7715 - val_loss: 1.5531 - val_accuracy: 0.4483\n",
      "Epoch 307/700\n",
      "106/106 [==============================] - 8s 79ms/step - loss: 0.6214 - accuracy: 0.7662 - val_loss: 1.6234 - val_accuracy: 0.4437\n",
      "Epoch 308/700\n",
      "106/106 [==============================] - 8s 79ms/step - loss: 0.6204 - accuracy: 0.7881 - val_loss: 1.5415 - val_accuracy: 0.4621\n",
      "Epoch 309/700\n",
      "106/106 [==============================] - 8s 80ms/step - loss: 0.6424 - accuracy: 0.7573 - val_loss: 1.5362 - val_accuracy: 0.4483\n",
      "Epoch 310/700\n",
      "106/106 [==============================] - 8s 80ms/step - loss: 0.6154 - accuracy: 0.7774 - val_loss: 1.5047 - val_accuracy: 0.4667\n",
      "Epoch 311/700\n",
      "106/106 [==============================] - 8s 79ms/step - loss: 0.6258 - accuracy: 0.7792 - val_loss: 1.5468 - val_accuracy: 0.4483\n",
      "Epoch 312/700\n",
      "106/106 [==============================] - 8s 79ms/step - loss: 0.6239 - accuracy: 0.7644 - val_loss: 1.5395 - val_accuracy: 0.4575\n",
      "Epoch 313/700\n",
      "106/106 [==============================] - 9s 81ms/step - loss: 0.6242 - accuracy: 0.7822 - val_loss: 1.5516 - val_accuracy: 0.4621\n",
      "Epoch 314/700\n",
      "106/106 [==============================] - 10s 95ms/step - loss: 0.6237 - accuracy: 0.7709 - val_loss: 1.5232 - val_accuracy: 0.4529\n",
      "Epoch 315/700\n",
      "106/106 [==============================] - 9s 82ms/step - loss: 0.6111 - accuracy: 0.7792 - val_loss: 1.6064 - val_accuracy: 0.4552\n",
      "Epoch 316/700\n",
      "106/106 [==============================] - 9s 82ms/step - loss: 0.6015 - accuracy: 0.7834 - val_loss: 1.5851 - val_accuracy: 0.4667\n",
      "Epoch 317/700\n",
      "106/106 [==============================] - 9s 84ms/step - loss: 0.5770 - accuracy: 0.7941 - val_loss: 1.6689 - val_accuracy: 0.4460\n",
      "Epoch 328/700\n",
      "106/106 [==============================] - 9s 83ms/step - loss: 0.5695 - accuracy: 0.7875 - val_loss: 1.7766 - val_accuracy: 0.4138\n",
      "Epoch 329/700\n",
      "106/106 [==============================] - 9s 84ms/step - loss: 0.5826 - accuracy: 0.7947 - val_loss: 1.5410 - val_accuracy: 0.4667\n",
      "Epoch 330/700\n",
      "106/106 [==============================] - 9s 83ms/step - loss: 0.5781 - accuracy: 0.7947 - val_loss: 1.5571 - val_accuracy: 0.4667\n",
      "Epoch 331/700\n",
      "106/106 [==============================] - 9s 84ms/step - loss: 0.5662 - accuracy: 0.7970 - val_loss: 1.5453 - val_accuracy: 0.4483\n",
      "Epoch 332/700\n",
      "106/106 [==============================] - 9s 84ms/step - loss: 0.5658 - accuracy: 0.7899 - val_loss: 1.5492 - val_accuracy: 0.4644\n",
      "Epoch 333/700\n",
      "106/106 [==============================] - 9s 84ms/step - loss: 0.5664 - accuracy: 0.8047 - val_loss: 1.6081 - val_accuracy: 0.4575\n",
      "Epoch 334/700\n",
      "106/106 [==============================] - 9s 84ms/step - loss: 0.5566 - accuracy: 0.7976 - val_loss: 1.6014 - val_accuracy: 0.4483\n",
      "Epoch 335/700\n",
      "106/106 [==============================] - 9s 83ms/step - loss: 0.5617 - accuracy: 0.7947 - val_loss: 1.6131 - val_accuracy: 0.4552\n",
      "Epoch 336/700\n",
      "106/106 [==============================] - 9s 84ms/step - loss: 0.5464 - accuracy: 0.8119 - val_loss: 1.6631 - val_accuracy: 0.4460\n",
      "Epoch 337/700\n",
      "106/106 [==============================] - 9s 83ms/step - loss: 0.5517 - accuracy: 0.7988 - val_loss: 1.5384 - val_accuracy: 0.4552\n",
      "Epoch 338/700\n",
      "106/106 [==============================] - 9s 87ms/step - loss: 0.5518 - accuracy: 0.8024 - val_loss: 1.5557 - val_accuracy: 0.4667\n",
      "Epoch 339/700\n",
      "106/106 [==============================] - 9s 85ms/step - loss: 0.5554 - accuracy: 0.7976 - val_loss: 1.6474 - val_accuracy: 0.4529\n",
      "Epoch 340/700\n",
      "106/106 [==============================] - 9s 87ms/step - loss: 0.5427 - accuracy: 0.8065 - val_loss: 1.6180 - val_accuracy: 0.4575\n",
      "Epoch 341/700\n",
      "106/106 [==============================] - 9s 85ms/step - loss: 0.5152 - accuracy: 0.8196 - val_loss: 1.7745 - val_accuracy: 0.4460\n",
      "Epoch 355/700\n",
      "106/106 [==============================] - 9s 85ms/step - loss: 0.5163 - accuracy: 0.8095 - val_loss: 1.6064 - val_accuracy: 0.4644\n",
      "Epoch 356/700\n",
      "106/106 [==============================] - 9s 85ms/step - loss: 0.5131 - accuracy: 0.8202 - val_loss: 1.6029 - val_accuracy: 0.4667\n",
      "Epoch 357/700\n",
      "106/106 [==============================] - 9s 84ms/step - loss: 0.5210 - accuracy: 0.8136 - val_loss: 1.6658 - val_accuracy: 0.4644\n",
      "Epoch 358/700\n",
      "106/106 [==============================] - 9s 83ms/step - loss: 0.4983 - accuracy: 0.8208 - val_loss: 1.6676 - val_accuracy: 0.4644\n",
      "Epoch 359/700\n",
      "106/106 [==============================] - 9s 85ms/step - loss: 0.4978 - accuracy: 0.8303 - val_loss: 1.6060 - val_accuracy: 0.4644\n",
      "Epoch 360/700\n",
      "106/106 [==============================] - 9s 85ms/step - loss: 0.5175 - accuracy: 0.8178 - val_loss: 1.6000 - val_accuracy: 0.4506\n",
      "Epoch 361/700\n",
      "106/106 [==============================] - 9s 84ms/step - loss: 0.5085 - accuracy: 0.8231 - val_loss: 1.6583 - val_accuracy: 0.4690\n",
      "Epoch 362/700\n",
      "106/106 [==============================] - 9s 85ms/step - loss: 0.5040 - accuracy: 0.8291 - val_loss: 1.6300 - val_accuracy: 0.4552\n",
      "Epoch 363/700\n",
      "106/106 [==============================] - 9s 84ms/step - loss: 0.5028 - accuracy: 0.8226 - val_loss: 1.6221 - val_accuracy: 0.4598\n",
      "Epoch 364/700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106/106 [==============================] - 9s 86ms/step - loss: 0.4928 - accuracy: 0.8409 - val_loss: 1.6601 - val_accuracy: 0.4713\n",
      "Epoch 365/700\n",
      "106/106 [==============================] - 9s 84ms/step - loss: 0.4813 - accuracy: 0.8350 - val_loss: 1.7739 - val_accuracy: 0.4414\n",
      "Epoch 366/700\n",
      "106/106 [==============================] - 9s 84ms/step - loss: 0.4929 - accuracy: 0.8202 - val_loss: 1.7166 - val_accuracy: 0.4621\n",
      "Epoch 367/700\n",
      "106/106 [==============================] - 9s 84ms/step - loss: 0.4663 - accuracy: 0.8326 - val_loss: 1.7372 - val_accuracy: 0.4644\n",
      "Epoch 368/700\n",
      "106/106 [==============================] - 9s 83ms/step - loss: 0.4928 - accuracy: 0.8178 - val_loss: 1.6718 - val_accuracy: 0.4621\n",
      "Epoch 369/700\n",
      "106/106 [==============================] - 9s 84ms/step - loss: 0.4862 - accuracy: 0.8273 - val_loss: 1.6427 - val_accuracy: 0.4690\n",
      "Epoch 370/700\n",
      "106/106 [==============================] - 9s 83ms/step - loss: 0.4750 - accuracy: 0.8338 - val_loss: 1.6424 - val_accuracy: 0.4690\n",
      "Epoch 371/700\n",
      "106/106 [==============================] - 9s 84ms/step - loss: 0.4503 - accuracy: 0.8398 - val_loss: 1.8269 - val_accuracy: 0.4506\n",
      "Epoch 384/700\n",
      "106/106 [==============================] - 9s 88ms/step - loss: 0.4413 - accuracy: 0.8605 - val_loss: 1.6607 - val_accuracy: 0.4575\n",
      "Epoch 385/700\n",
      "106/106 [==============================] - 9s 85ms/step - loss: 0.4518 - accuracy: 0.8463 - val_loss: 1.6552 - val_accuracy: 0.4644\n",
      "Epoch 386/700\n",
      "106/106 [==============================] - 9s 85ms/step - loss: 0.4373 - accuracy: 0.8475 - val_loss: 1.7828 - val_accuracy: 0.4667\n",
      "Epoch 387/700\n",
      "106/106 [==============================] - 9s 85ms/step - loss: 0.4522 - accuracy: 0.8463 - val_loss: 1.6964 - val_accuracy: 0.4644\n",
      "Epoch 388/700\n",
      "106/106 [==============================] - 9s 85ms/step - loss: 0.4276 - accuracy: 0.8493 - val_loss: 1.7351 - val_accuracy: 0.4644\n",
      "Epoch 389/700\n",
      "106/106 [==============================] - 9s 84ms/step - loss: 0.4424 - accuracy: 0.8481 - val_loss: 1.6873 - val_accuracy: 0.4575\n",
      "Epoch 390/700\n",
      "106/106 [==============================] - 9s 85ms/step - loss: 0.4289 - accuracy: 0.8564 - val_loss: 1.6735 - val_accuracy: 0.4713\n",
      "Epoch 391/700\n",
      "106/106 [==============================] - 9s 84ms/step - loss: 0.4501 - accuracy: 0.8439 - val_loss: 1.9003 - val_accuracy: 0.4276\n",
      "Epoch 392/700\n",
      "106/106 [==============================] - 9s 84ms/step - loss: 0.4303 - accuracy: 0.8522 - val_loss: 1.8552 - val_accuracy: 0.4575\n",
      "Epoch 393/700\n",
      "106/106 [==============================] - 9s 82ms/step - loss: 0.4307 - accuracy: 0.8546 - val_loss: 1.6766 - val_accuracy: 0.4460\n",
      "Epoch 394/700\n",
      "106/106 [==============================] - 9s 81ms/step - loss: 0.4369 - accuracy: 0.8439 - val_loss: 1.8210 - val_accuracy: 0.4575\n",
      "Epoch 395/700\n",
      "106/106 [==============================] - 9s 83ms/step - loss: 0.4263 - accuracy: 0.8463 - val_loss: 1.8170 - val_accuracy: 0.4552\n",
      "Epoch 396/700\n",
      "106/106 [==============================] - 9s 82ms/step - loss: 0.4121 - accuracy: 0.8504 - val_loss: 1.8003 - val_accuracy: 0.4621\n",
      "Epoch 397/700\n",
      "106/106 [==============================] - 9s 84ms/step - loss: 0.4207 - accuracy: 0.8552 - val_loss: 1.6860 - val_accuracy: 0.4713\n",
      "Epoch 398/700\n",
      "106/106 [==============================] - 8s 80ms/step - loss: 0.4252 - accuracy: 0.8558 - val_loss: 1.6945 - val_accuracy: 0.4529\n",
      "Epoch 399/700\n",
      "106/106 [==============================] - 8s 80ms/step - loss: 0.3914 - accuracy: 0.8576 - val_loss: 1.7908 - val_accuracy: 0.4552\n",
      "Epoch 414/700\n",
      "106/106 [==============================] - 8s 80ms/step - loss: 0.3862 - accuracy: 0.8688 - val_loss: 1.8021 - val_accuracy: 0.4529\n",
      "Epoch 415/700\n",
      "106/106 [==============================] - 8s 80ms/step - loss: 0.3755 - accuracy: 0.8789 - val_loss: 1.7791 - val_accuracy: 0.4598\n",
      "Epoch 416/700\n",
      "106/106 [==============================] - 9s 86ms/step - loss: 0.3783 - accuracy: 0.8789 - val_loss: 1.7853 - val_accuracy: 0.4598\n",
      "Epoch 417/700\n",
      "106/106 [==============================] - 9s 83ms/step - loss: 0.3967 - accuracy: 0.8552 - val_loss: 1.7480 - val_accuracy: 0.4759\n",
      "Epoch 418/700\n",
      "106/106 [==============================] - 9s 87ms/step - loss: 0.3909 - accuracy: 0.8682 - val_loss: 1.8019 - val_accuracy: 0.4736\n",
      "Epoch 419/700\n",
      "106/106 [==============================] - 9s 84ms/step - loss: 0.3922 - accuracy: 0.8647 - val_loss: 1.8767 - val_accuracy: 0.4552\n",
      "Epoch 420/700\n",
      "106/106 [==============================] - 9s 85ms/step - loss: 0.3919 - accuracy: 0.8682 - val_loss: 1.7687 - val_accuracy: 0.4690\n",
      "Epoch 421/700\n",
      "106/106 [==============================] - 9s 84ms/step - loss: 0.3761 - accuracy: 0.8653 - val_loss: 1.8217 - val_accuracy: 0.4575\n",
      "Epoch 422/700\n",
      "106/106 [==============================] - 9s 84ms/step - loss: 0.3897 - accuracy: 0.8754 - val_loss: 1.8107 - val_accuracy: 0.4644\n",
      "Epoch 423/700\n",
      "106/106 [==============================] - 9s 86ms/step - loss: 0.3855 - accuracy: 0.8700 - val_loss: 1.7513 - val_accuracy: 0.4644\n",
      "Epoch 424/700\n",
      "106/106 [==============================] - 9s 85ms/step - loss: 0.3842 - accuracy: 0.8712 - val_loss: 1.7747 - val_accuracy: 0.4644\n",
      "Epoch 425/700\n",
      "106/106 [==============================] - 9s 83ms/step - loss: 0.3830 - accuracy: 0.8700 - val_loss: 1.8114 - val_accuracy: 0.4690\n",
      "Epoch 426/700\n",
      "106/106 [==============================] - 9s 82ms/step - loss: 0.3655 - accuracy: 0.8801 - val_loss: 1.7716 - val_accuracy: 0.4713\n",
      "Epoch 427/700\n",
      "106/106 [==============================] - 9s 84ms/step - loss: 0.3737 - accuracy: 0.8724 - val_loss: 1.7735 - val_accuracy: 0.4644\n",
      "Epoch 428/700\n",
      "106/106 [==============================] - 9s 84ms/step - loss: 0.3734 - accuracy: 0.8724 - val_loss: 1.8113 - val_accuracy: 0.4690\n",
      "Epoch 429/700\n",
      "106/106 [==============================] - 9s 87ms/step - loss: 0.3650 - accuracy: 0.8766 - val_loss: 1.7961 - val_accuracy: 0.4598\n",
      "Epoch 430/700\n",
      "106/106 [==============================] - 9s 81ms/step - loss: 0.3373 - accuracy: 0.8789 - val_loss: 1.8382 - val_accuracy: 0.4598\n",
      "Epoch 446/700\n",
      "106/106 [==============================] - 9s 82ms/step - loss: 0.3127 - accuracy: 0.8932 - val_loss: 1.8757 - val_accuracy: 0.4552\n",
      "Epoch 447/700\n",
      "106/106 [==============================] - 9s 85ms/step - loss: 0.3235 - accuracy: 0.8896 - val_loss: 1.8123 - val_accuracy: 0.4598\n",
      "Epoch 448/700\n",
      "106/106 [==============================] - 9s 83ms/step - loss: 0.3226 - accuracy: 0.8973 - val_loss: 1.9085 - val_accuracy: 0.4644\n",
      "Epoch 449/700\n",
      "106/106 [==============================] - 9s 83ms/step - loss: 0.3477 - accuracy: 0.8831 - val_loss: 1.8661 - val_accuracy: 0.4483\n",
      "Epoch 450/700\n",
      "106/106 [==============================] - 9s 82ms/step - loss: 0.3309 - accuracy: 0.8967 - val_loss: 1.8628 - val_accuracy: 0.4460\n",
      "Epoch 451/700\n",
      "106/106 [==============================] - 9s 82ms/step - loss: 0.3424 - accuracy: 0.8783 - val_loss: 1.8727 - val_accuracy: 0.4644\n",
      "Epoch 452/700\n",
      "106/106 [==============================] - 9s 84ms/step - loss: 0.3332 - accuracy: 0.8908 - val_loss: 1.8431 - val_accuracy: 0.4713\n",
      "Epoch 453/700\n",
      "106/106 [==============================] - 9s 84ms/step - loss: 0.3290 - accuracy: 0.8896 - val_loss: 1.8984 - val_accuracy: 0.4690\n",
      "Epoch 454/700\n",
      "106/106 [==============================] - 9s 84ms/step - loss: 0.3280 - accuracy: 0.8944 - val_loss: 1.8676 - val_accuracy: 0.4644\n",
      "Epoch 455/700\n",
      "106/106 [==============================] - 9s 85ms/step - loss: 0.3228 - accuracy: 0.8920 - val_loss: 1.9245 - val_accuracy: 0.4552\n",
      "Epoch 456/700\n",
      "106/106 [==============================] - 9s 85ms/step - loss: 0.3116 - accuracy: 0.8985 - val_loss: 1.8759 - val_accuracy: 0.4621\n",
      "Epoch 457/700\n",
      "106/106 [==============================] - 9s 84ms/step - loss: 0.3067 - accuracy: 0.9003 - val_loss: 2.0011 - val_accuracy: 0.4575\n",
      "Epoch 458/700\n",
      "106/106 [==============================] - 9s 87ms/step - loss: 0.3032 - accuracy: 0.8926 - val_loss: 1.8596 - val_accuracy: 0.4713\n",
      "Epoch 459/700\n",
      "106/106 [==============================] - 9s 84ms/step - loss: 0.3224 - accuracy: 0.8884 - val_loss: 1.9201 - val_accuracy: 0.4575\n",
      "Epoch 460/700\n",
      "106/106 [==============================] - 9s 83ms/step - loss: 0.2949 - accuracy: 0.9092 - val_loss: 1.9877 - val_accuracy: 0.4621\n",
      "Epoch 461/700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106/106 [==============================] - 9s 85ms/step - loss: 0.3095 - accuracy: 0.8866 - val_loss: 1.9077 - val_accuracy: 0.4575\n",
      "Epoch 462/700\n",
      "106/106 [==============================] - 9s 88ms/step - loss: 0.3213 - accuracy: 0.8855 - val_loss: 1.8376 - val_accuracy: 0.4644\n",
      "Epoch 463/700\n",
      "106/106 [==============================] - 9s 85ms/step - loss: 0.3178 - accuracy: 0.8961 - val_loss: 1.8894 - val_accuracy: 0.4644\n",
      "Epoch 464/700\n",
      "106/106 [==============================] - 9s 85ms/step - loss: 0.2614 - accuracy: 0.9282 - val_loss: 1.9044 - val_accuracy: 0.4621\n",
      "Epoch 473/700\n",
      "106/106 [==============================] - 9s 85ms/step - loss: 0.2932 - accuracy: 0.8973 - val_loss: 1.9325 - val_accuracy: 0.4667\n",
      "Epoch 474/700\n",
      "106/106 [==============================] - 9s 87ms/step - loss: 0.2924 - accuracy: 0.9056 - val_loss: 1.9723 - val_accuracy: 0.4598\n",
      "Epoch 475/700\n",
      "106/106 [==============================] - 9s 87ms/step - loss: 0.2959 - accuracy: 0.8979 - val_loss: 1.9558 - val_accuracy: 0.4690\n",
      "Epoch 476/700\n",
      "106/106 [==============================] - 9s 82ms/step - loss: 0.2789 - accuracy: 0.9163 - val_loss: 1.9788 - val_accuracy: 0.4437\n",
      "Epoch 477/700\n",
      "106/106 [==============================] - 9s 83ms/step - loss: 0.2640 - accuracy: 0.9151 - val_loss: 1.9826 - val_accuracy: 0.4736\n",
      "Epoch 478/700\n",
      "106/106 [==============================] - 9s 83ms/step - loss: 0.2909 - accuracy: 0.9080 - val_loss: 1.9564 - val_accuracy: 0.4644\n",
      "Epoch 479/700\n",
      "106/106 [==============================] - 9s 84ms/step - loss: 0.2784 - accuracy: 0.9098 - val_loss: 1.9671 - val_accuracy: 0.4552\n",
      "Epoch 480/700\n",
      "106/106 [==============================] - 9s 87ms/step - loss: 0.2748 - accuracy: 0.9134 - val_loss: 1.9910 - val_accuracy: 0.4598\n",
      "Epoch 481/700\n",
      "106/106 [==============================] - 9s 84ms/step - loss: 0.2690 - accuracy: 0.9110 - val_loss: 2.1125 - val_accuracy: 0.4437\n",
      "Epoch 482/700\n",
      "106/106 [==============================] - 9s 82ms/step - loss: 0.2751 - accuracy: 0.9134 - val_loss: 1.9286 - val_accuracy: 0.4667\n",
      "Epoch 483/700\n",
      "106/106 [==============================] - 9s 84ms/step - loss: 0.2746 - accuracy: 0.9139 - val_loss: 1.9902 - val_accuracy: 0.4713\n",
      "Epoch 484/700\n",
      "106/106 [==============================] - 9s 83ms/step - loss: 0.2764 - accuracy: 0.9116 - val_loss: 1.9404 - val_accuracy: 0.4759\n",
      "Epoch 485/700\n",
      "106/106 [==============================] - 9s 85ms/step - loss: 0.2606 - accuracy: 0.9151 - val_loss: 1.9884 - val_accuracy: 0.4621\n",
      "Epoch 486/700\n",
      "106/106 [==============================] - 9s 87ms/step - loss: 0.2621 - accuracy: 0.9145 - val_loss: 1.9559 - val_accuracy: 0.4713\n",
      "Epoch 487/700\n",
      "106/106 [==============================] - 9s 82ms/step - loss: 0.2756 - accuracy: 0.9050 - val_loss: 2.0360 - val_accuracy: 0.4644\n",
      "Epoch 488/700\n",
      "106/106 [==============================] - 9s 82ms/step - loss: 0.2793 - accuracy: 0.9098 - val_loss: 2.0021 - val_accuracy: 0.4644\n",
      "Epoch 489/700\n",
      "106/106 [==============================] - 9s 85ms/step - loss: 0.2344 - accuracy: 0.9199 - val_loss: 2.1447 - val_accuracy: 0.4598\n",
      "Epoch 509/700\n",
      "106/106 [==============================] - 9s 83ms/step - loss: 0.2390 - accuracy: 0.9205 - val_loss: 2.0822 - val_accuracy: 0.4713\n",
      "Epoch 510/700\n",
      "106/106 [==============================] - 9s 85ms/step - loss: 0.2331 - accuracy: 0.9228 - val_loss: 2.0447 - val_accuracy: 0.4644\n",
      "Epoch 511/700\n",
      "106/106 [==============================] - 9s 83ms/step - loss: 0.2363 - accuracy: 0.9270 - val_loss: 2.1011 - val_accuracy: 0.4621\n",
      "Epoch 512/700\n",
      "106/106 [==============================] - 9s 81ms/step - loss: 0.2335 - accuracy: 0.9258 - val_loss: 2.0650 - val_accuracy: 0.4782\n",
      "Epoch 513/700\n",
      "106/106 [==============================] - 10s 92ms/step - loss: 0.2449 - accuracy: 0.9145 - val_loss: 2.1002 - val_accuracy: 0.4759\n",
      "Epoch 514/700\n",
      "106/106 [==============================] - 9s 83ms/step - loss: 0.2242 - accuracy: 0.9323 - val_loss: 2.1469 - val_accuracy: 0.4667\n",
      "Epoch 515/700\n",
      "106/106 [==============================] - 9s 84ms/step - loss: 0.2269 - accuracy: 0.9300 - val_loss: 2.1101 - val_accuracy: 0.4805\n",
      "Epoch 516/700\n",
      "106/106 [==============================] - 10s 92ms/step - loss: 0.2344 - accuracy: 0.9223 - val_loss: 2.1967 - val_accuracy: 0.4506\n",
      "Epoch 517/700\n",
      "106/106 [==============================] - 9s 89ms/step - loss: 0.2248 - accuracy: 0.9288 - val_loss: 2.0847 - val_accuracy: 0.4782\n",
      "Epoch 518/700\n",
      "106/106 [==============================] - 9s 87ms/step - loss: 0.2250 - accuracy: 0.9347 - val_loss: 2.0773 - val_accuracy: 0.4805\n",
      "Epoch 519/700\n",
      "106/106 [==============================] - 9s 84ms/step - loss: 0.2266 - accuracy: 0.9264 - val_loss: 2.0888 - val_accuracy: 0.4828\n",
      "Epoch 520/700\n",
      "106/106 [==============================] - 9s 82ms/step - loss: 0.2295 - accuracy: 0.9282 - val_loss: 2.1063 - val_accuracy: 0.4805\n",
      "Epoch 521/700\n",
      "106/106 [==============================] - 9s 84ms/step - loss: 0.2338 - accuracy: 0.9228 - val_loss: 2.2375 - val_accuracy: 0.4506\n",
      "Epoch 522/700\n",
      "106/106 [==============================] - 10s 95ms/step - loss: 0.1912 - accuracy: 0.9383 - val_loss: 2.1394 - val_accuracy: 0.4690\n",
      "Epoch 539/700\n",
      "106/106 [==============================] - 10s 90ms/step - loss: 0.2020 - accuracy: 0.9424 - val_loss: 2.1573 - val_accuracy: 0.4690\n",
      "Epoch 540/700\n",
      "106/106 [==============================] - 11s 102ms/step - loss: 0.2080 - accuracy: 0.9383 - val_loss: 2.1562 - val_accuracy: 0.4736\n",
      "Epoch 541/700\n",
      "106/106 [==============================] - 9s 88ms/step - loss: 0.1854 - accuracy: 0.9418 - val_loss: 2.2206 - val_accuracy: 0.4713\n",
      "Epoch 542/700\n",
      "106/106 [==============================] - 9s 84ms/step - loss: 0.1968 - accuracy: 0.9335 - val_loss: 2.3197 - val_accuracy: 0.4552\n",
      "Epoch 543/700\n",
      "106/106 [==============================] - 9s 86ms/step - loss: 0.1947 - accuracy: 0.9430 - val_loss: 2.1914 - val_accuracy: 0.4828\n",
      "Epoch 544/700\n",
      "106/106 [==============================] - 9s 83ms/step - loss: 0.1960 - accuracy: 0.9448 - val_loss: 2.3492 - val_accuracy: 0.4414\n",
      "Epoch 545/700\n",
      "106/106 [==============================] - 9s 84ms/step - loss: 0.1989 - accuracy: 0.9418 - val_loss: 2.2870 - val_accuracy: 0.4644\n",
      "Epoch 546/700\n",
      "106/106 [==============================] - 9s 85ms/step - loss: 0.1832 - accuracy: 0.9460 - val_loss: 2.1994 - val_accuracy: 0.4782\n",
      "Epoch 547/700\n",
      "106/106 [==============================] - 9s 84ms/step - loss: 0.1853 - accuracy: 0.9472 - val_loss: 2.3814 - val_accuracy: 0.4391\n",
      "Epoch 548/700\n",
      "106/106 [==============================] - 9s 83ms/step - loss: 0.1843 - accuracy: 0.9418 - val_loss: 2.1910 - val_accuracy: 0.4828\n",
      "Epoch 549/700\n",
      "106/106 [==============================] - 9s 86ms/step - loss: 0.1864 - accuracy: 0.9412 - val_loss: 2.2241 - val_accuracy: 0.4644\n",
      "Epoch 550/700\n",
      "106/106 [==============================] - 10s 90ms/step - loss: 0.1790 - accuracy: 0.9407 - val_loss: 2.2032 - val_accuracy: 0.4851\n",
      "Epoch 551/700\n",
      "106/106 [==============================] - 9s 85ms/step - loss: 0.1865 - accuracy: 0.9430 - val_loss: 2.2102 - val_accuracy: 0.4805\n",
      "Epoch 552/700\n",
      "106/106 [==============================] - 9s 82ms/step - loss: 0.1745 - accuracy: 0.9478 - val_loss: 2.2349 - val_accuracy: 0.4713\n",
      "Epoch 553/700\n",
      "106/106 [==============================] - 9s 83ms/step - loss: 0.1920 - accuracy: 0.9383 - val_loss: 2.2650 - val_accuracy: 0.4552\n",
      "Epoch 554/700\n",
      "106/106 [==============================] - 9s 82ms/step - loss: 0.1696 - accuracy: 0.9484 - val_loss: 2.2411 - val_accuracy: 0.4713\n",
      "Epoch 555/700\n",
      "106/106 [==============================] - 9s 86ms/step - loss: 0.1426 - accuracy: 0.9573 - val_loss: 2.2982 - val_accuracy: 0.4782\n",
      "Epoch 574/700\n",
      "106/106 [==============================] - 9s 81ms/step - loss: 0.1706 - accuracy: 0.9436 - val_loss: 2.3829 - val_accuracy: 0.4575\n",
      "Epoch 575/700\n",
      "106/106 [==============================] - 9s 84ms/step - loss: 0.1536 - accuracy: 0.9507 - val_loss: 2.4558 - val_accuracy: 0.4690\n",
      "Epoch 608/700\n",
      "106/106 [==============================] - 8s 77ms/step - loss: 0.1370 - accuracy: 0.9602 - val_loss: 2.5566 - val_accuracy: 0.4598\n",
      "Epoch 609/700\n",
      "106/106 [==============================] - 8s 79ms/step - loss: 0.1302 - accuracy: 0.9602 - val_loss: 2.5010 - val_accuracy: 0.4759\n",
      "Epoch 610/700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106/106 [==============================] - 8s 79ms/step - loss: 0.1249 - accuracy: 0.9644 - val_loss: 2.5845 - val_accuracy: 0.4667\n",
      "Epoch 611/700\n",
      "106/106 [==============================] - 8s 73ms/step - loss: 0.1342 - accuracy: 0.9638 - val_loss: 2.5256 - val_accuracy: 0.4598\n",
      "Epoch 612/700\n",
      "106/106 [==============================] - 8s 78ms/step - loss: 0.1242 - accuracy: 0.9585 - val_loss: 2.6255 - val_accuracy: 0.4460\n",
      "Epoch 613/700\n",
      "106/106 [==============================] - 9s 86ms/step - loss: 0.1324 - accuracy: 0.9650 - val_loss: 2.5281 - val_accuracy: 0.4736\n",
      "Epoch 614/700\n",
      "106/106 [==============================] - 8s 77ms/step - loss: 0.1346 - accuracy: 0.9602 - val_loss: 2.4598 - val_accuracy: 0.4736\n",
      "Epoch 615/700\n",
      "106/106 [==============================] - 8s 78ms/step - loss: 0.1249 - accuracy: 0.9626 - val_loss: 2.5190 - val_accuracy: 0.4782\n",
      "Epoch 616/700\n",
      "106/106 [==============================] - 9s 83ms/step - loss: 0.1229 - accuracy: 0.9614 - val_loss: 2.6522 - val_accuracy: 0.4598\n",
      "Epoch 617/700\n",
      "106/106 [==============================] - 8s 78ms/step - loss: 0.1177 - accuracy: 0.9680 - val_loss: 2.5659 - val_accuracy: 0.4736\n",
      "Epoch 618/700\n",
      "106/106 [==============================] - 9s 87ms/step - loss: 0.1230 - accuracy: 0.9632 - val_loss: 2.5692 - val_accuracy: 0.4483\n",
      "Epoch 619/700\n",
      "106/106 [==============================] - 8s 78ms/step - loss: 0.1257 - accuracy: 0.9668 - val_loss: 2.5593 - val_accuracy: 0.4667\n",
      "Epoch 620/700\n",
      "106/106 [==============================] - 8s 77ms/step - loss: 0.1297 - accuracy: 0.9591 - val_loss: 2.6797 - val_accuracy: 0.4437\n",
      "Epoch 621/700\n",
      "106/106 [==============================] - 8s 75ms/step - loss: 0.1328 - accuracy: 0.9608 - val_loss: 2.4859 - val_accuracy: 0.4828\n",
      "Epoch 622/700\n",
      "106/106 [==============================] - 8s 75ms/step - loss: 0.1126 - accuracy: 0.9680 - val_loss: 2.5426 - val_accuracy: 0.4644\n",
      "Epoch 623/700\n",
      "106/106 [==============================] - 8s 75ms/step - loss: 0.1169 - accuracy: 0.9703 - val_loss: 2.5424 - val_accuracy: 0.4805\n",
      "Epoch 624/700\n",
      "106/106 [==============================] - 8s 75ms/step - loss: 0.1189 - accuracy: 0.9685 - val_loss: 2.5748 - val_accuracy: 0.4506\n",
      "Epoch 625/700\n",
      "106/106 [==============================] - 8s 76ms/step - loss: 0.1178 - accuracy: 0.9620 - val_loss: 2.5946 - val_accuracy: 0.4644\n",
      "Epoch 626/700\n",
      "106/106 [==============================] - 8s 76ms/step - loss: 0.1230 - accuracy: 0.9585 - val_loss: 2.5152 - val_accuracy: 0.4736\n",
      "Epoch 627/700\n",
      "106/106 [==============================] - 8s 78ms/step - loss: 0.1121 - accuracy: 0.9662 - val_loss: 2.5064 - val_accuracy: 0.4897\n",
      "Epoch 628/700\n",
      "106/106 [==============================] - 9s 83ms/step - loss: 0.1115 - accuracy: 0.9674 - val_loss: 2.5342 - val_accuracy: 0.4805\n",
      "Epoch 629/700\n",
      "106/106 [==============================] - 8s 77ms/step - loss: 0.1122 - accuracy: 0.9662 - val_loss: 2.5864 - val_accuracy: 0.4851\n",
      "Epoch 630/700\n",
      "106/106 [==============================] - 8s 76ms/step - loss: 0.1251 - accuracy: 0.9608 - val_loss: 2.7464 - val_accuracy: 0.4575\n",
      "Epoch 631/700\n",
      "106/106 [==============================] - 9s 80ms/step - loss: 0.1174 - accuracy: 0.9591 - val_loss: 2.5189 - val_accuracy: 0.4736\n",
      "Epoch 632/700\n",
      "106/106 [==============================] - 8s 77ms/step - loss: 0.1160 - accuracy: 0.9691 - val_loss: 2.5528 - val_accuracy: 0.4713\n",
      "Epoch 633/700\n",
      "106/106 [==============================] - 9s 81ms/step - loss: 0.1148 - accuracy: 0.9691 - val_loss: 2.4955 - val_accuracy: 0.4828\n",
      "Epoch 634/700\n",
      "106/106 [==============================] - 9s 82ms/step - loss: 0.1196 - accuracy: 0.9638 - val_loss: 2.5039 - val_accuracy: 0.4713\n",
      "Epoch 635/700\n",
      "106/106 [==============================] - 8s 79ms/step - loss: 0.1209 - accuracy: 0.9650 - val_loss: 2.6132 - val_accuracy: 0.4644\n",
      "Epoch 636/700\n",
      "106/106 [==============================] - 8s 75ms/step - loss: 0.1049 - accuracy: 0.9715 - val_loss: 2.5837 - val_accuracy: 0.4713\n",
      "Epoch 637/700\n",
      "106/106 [==============================] - 8s 77ms/step - loss: 0.1104 - accuracy: 0.9703 - val_loss: 2.5660 - val_accuracy: 0.4828\n",
      "Epoch 638/700\n",
      "106/106 [==============================] - 8s 77ms/step - loss: 0.1132 - accuracy: 0.9739 - val_loss: 2.6507 - val_accuracy: 0.4644\n",
      "Epoch 639/700\n",
      "106/106 [==============================] - 8s 76ms/step - loss: 0.1068 - accuracy: 0.9709 - val_loss: 2.6626 - val_accuracy: 0.4460\n",
      "Epoch 640/700\n",
      "106/106 [==============================] - 8s 75ms/step - loss: 0.1117 - accuracy: 0.9685 - val_loss: 2.6251 - val_accuracy: 0.4759\n",
      "Epoch 641/700\n",
      "106/106 [==============================] - 8s 74ms/step - loss: 0.1021 - accuracy: 0.9709 - val_loss: 2.6091 - val_accuracy: 0.4874\n",
      "Epoch 654/700\n",
      "106/106 [==============================] - 8s 76ms/step - loss: 0.1003 - accuracy: 0.9721 - val_loss: 2.6514 - val_accuracy: 0.4736\n",
      "Epoch 655/700\n",
      "106/106 [==============================] - 8s 74ms/step - loss: 0.0889 - accuracy: 0.9780 - val_loss: 2.7087 - val_accuracy: 0.4552\n",
      "Epoch 656/700\n",
      "106/106 [==============================] - 8s 74ms/step - loss: 0.0967 - accuracy: 0.9727 - val_loss: 2.7539 - val_accuracy: 0.4598\n",
      "Epoch 657/700\n",
      "106/106 [==============================] - 8s 75ms/step - loss: 0.0996 - accuracy: 0.9685 - val_loss: 2.6745 - val_accuracy: 0.4736\n",
      "Epoch 658/700\n",
      "106/106 [==============================] - 8s 74ms/step - loss: 0.0992 - accuracy: 0.9727 - val_loss: 2.7611 - val_accuracy: 0.4575\n",
      "Epoch 659/700\n",
      "106/106 [==============================] - 8s 74ms/step - loss: 0.0871 - accuracy: 0.9769 - val_loss: 2.6860 - val_accuracy: 0.4805\n",
      "Epoch 660/700\n",
      "106/106 [==============================] - 8s 74ms/step - loss: 0.1038 - accuracy: 0.9668 - val_loss: 2.7022 - val_accuracy: 0.4759\n",
      "Epoch 661/700\n",
      "106/106 [==============================] - 8s 74ms/step - loss: 0.0983 - accuracy: 0.9757 - val_loss: 2.6799 - val_accuracy: 0.4713\n",
      "Epoch 662/700\n",
      "106/106 [==============================] - 8s 77ms/step - loss: 0.0912 - accuracy: 0.9804 - val_loss: 2.7546 - val_accuracy: 0.4644\n",
      "Epoch 663/700\n",
      "106/106 [==============================] - 9s 81ms/step - loss: 0.0989 - accuracy: 0.9697 - val_loss: 2.7055 - val_accuracy: 0.4713\n",
      "Epoch 664/700\n",
      "106/106 [==============================] - 8s 79ms/step - loss: 0.0975 - accuracy: 0.9721 - val_loss: 2.6961 - val_accuracy: 0.4805\n",
      "Epoch 665/700\n",
      "106/106 [==============================] - 8s 74ms/step - loss: 0.0922 - accuracy: 0.9733 - val_loss: 3.2391 - val_accuracy: 0.4253\n",
      "Epoch 666/700\n",
      "106/106 [==============================] - 8s 76ms/step - loss: 0.0812 - accuracy: 0.9816 - val_loss: 2.7560 - val_accuracy: 0.4575\n",
      "Epoch 679/700\n",
      "106/106 [==============================] - 8s 75ms/step - loss: 0.0918 - accuracy: 0.9721 - val_loss: 2.7509 - val_accuracy: 0.4713\n",
      "Epoch 680/700\n",
      "106/106 [==============================] - 8s 75ms/step - loss: 0.0831 - accuracy: 0.9798 - val_loss: 2.8520 - val_accuracy: 0.4460\n",
      "Epoch 681/700\n",
      "106/106 [==============================] - 8s 77ms/step - loss: 0.0814 - accuracy: 0.9774 - val_loss: 2.7736 - val_accuracy: 0.4621\n",
      "Epoch 682/700\n",
      "106/106 [==============================] - 8s 77ms/step - loss: 0.0882 - accuracy: 0.9751 - val_loss: 2.7907 - val_accuracy: 0.4690\n",
      "Epoch 683/700\n",
      "106/106 [==============================] - 8s 74ms/step - loss: 0.0878 - accuracy: 0.9715 - val_loss: 2.8203 - val_accuracy: 0.4690\n",
      "Epoch 684/700\n",
      "106/106 [==============================] - 8s 75ms/step - loss: 0.0879 - accuracy: 0.9721 - val_loss: 2.8128 - val_accuracy: 0.4598\n",
      "Epoch 685/700\n",
      "106/106 [==============================] - 8s 74ms/step - loss: 0.0894 - accuracy: 0.9739 - val_loss: 2.7956 - val_accuracy: 0.4644\n",
      "Epoch 686/700\n",
      "106/106 [==============================] - 8s 74ms/step - loss: 0.0842 - accuracy: 0.9739 - val_loss: 2.7627 - val_accuracy: 0.4690\n",
      "Epoch 687/700\n",
      "106/106 [==============================] - 8s 74ms/step - loss: 0.0927 - accuracy: 0.9697 - val_loss: 2.7442 - val_accuracy: 0.4782\n",
      "Epoch 688/700\n",
      "106/106 [==============================] - 8s 74ms/step - loss: 0.0773 - accuracy: 0.9798 - val_loss: 2.8437 - val_accuracy: 0.4621\n",
      "Epoch 689/700\n",
      "106/106 [==============================] - 8s 74ms/step - loss: 0.0893 - accuracy: 0.9769 - val_loss: 2.8639 - val_accuracy: 0.4575\n",
      "Epoch 690/700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106/106 [==============================] - 8s 75ms/step - loss: 0.0873 - accuracy: 0.9774 - val_loss: 2.8505 - val_accuracy: 0.4690\n",
      "Epoch 691/700\n",
      "106/106 [==============================] - 8s 79ms/step - loss: 0.0783 - accuracy: 0.9769 - val_loss: 2.7928 - val_accuracy: 0.4782\n",
      "Epoch 692/700\n",
      "106/106 [==============================] - 8s 78ms/step - loss: 0.0865 - accuracy: 0.9739 - val_loss: 2.8218 - val_accuracy: 0.4690\n",
      "Epoch 693/700\n",
      "106/106 [==============================] - 8s 76ms/step - loss: 0.0747 - accuracy: 0.9816 - val_loss: 2.8350 - val_accuracy: 0.4667\n",
      "Epoch 694/700\n",
      "106/106 [==============================] - 8s 76ms/step - loss: 0.0811 - accuracy: 0.9774 - val_loss: 2.8347 - val_accuracy: 0.4805\n",
      "Epoch 695/700\n",
      "106/106 [==============================] - 8s 77ms/step - loss: 0.0798 - accuracy: 0.9792 - val_loss: 2.8181 - val_accuracy: 0.4736\n",
      "Epoch 696/700\n",
      " 28/106 [======>.......................] - ETA: 5s - loss: 0.0758 - accuracy: 0.9777"
     ]
    }
   ],
   "source": [
    "cnnhistory=model.fit(x_traincnn, y_train, batch_size=16, epochs=700, validation_data=(x_testcnn, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(cnnhistory.history['loss'])\n",
    "plt.plot(cnnhistory.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved trained model at C:\\Users\\uber\\Desktop\\faurecia\\Mood_recognition\\saved_models\\Emotion_Voice_Detection_Model.h5 \n"
     ]
    }
   ],
   "source": [
    "model_name = 'Emotion_Voice_Detection_Model.h5'\n",
    "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
    "# Save model and weights\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "model_path = os.path.join(save_dir, model_name)\n",
    "model.save(model_path)\n",
    "print('Saved trained model at %s ' % model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "model_json = model.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    c:\\users\\uber\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1224 test_function  *\n        return step_function(self, iterator)\n    c:\\users\\uber\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1215 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    c:\\users\\uber\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1211 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    c:\\users\\uber\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2585 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    c:\\users\\uber\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2945 _call_for_each_replica\n        return fn(*args, **kwargs)\n    c:\\users\\uber\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1208 run_step  **\n        outputs = model.test_step(data)\n    c:\\users\\uber\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1176 test_step\n        self.compiled_loss(\n    c:\\users\\uber\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\compile_utils.py:204 __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    c:\\users\\uber\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\keras\\losses.py:149 __call__\n        losses = ag_call(y_true, y_pred)\n    c:\\users\\uber\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\keras\\losses.py:253 call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    c:\\users\\uber\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    c:\\users\\uber\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\keras\\losses.py:1535 categorical_crossentropy\n        return K.categorical_crossentropy(y_true, y_pred, from_logits=from_logits)\n    c:\\users\\uber\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    c:\\users\\uber\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py:4687 categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n    c:\\users\\uber\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_shape.py:1134 assert_is_compatible_with\n        raise ValueError(\"Shapes %s and %s are incompatible\" % (self, other))\n\n    ValueError: Shapes (None, 16) and (None, 10) are incompatible\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-2040cc4a3ddc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;31m# evaluate loaded model on test data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mloaded_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'categorical_crossentropy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mopt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloaded_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_testcnn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"%s: %.2f%%\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mloaded_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics_names\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\uber\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\uber\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict)\u001b[0m\n\u001b[0;32m   1377\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'TraceContext'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgraph_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'test'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep_num\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1378\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_test_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1379\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1380\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1381\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\uber\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    778\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\uber\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    821\u001b[0m       \u001b[1;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    822\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 823\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    824\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    825\u001b[0m       \u001b[1;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\uber\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[1;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[0;32m    694\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph_deleter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFunctionDeleter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lifted_initializer_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    695\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m--> 696\u001b[1;33m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0m\u001b[0;32m    697\u001b[0m             *args, **kwds))\n\u001b[0;32m    698\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\uber\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2853\u001b[0m       \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2854\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2855\u001b[1;33m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2856\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2857\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\uber\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   3211\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3212\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3213\u001b[1;33m       \u001b[0mgraph_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3214\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3215\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\uber\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   3063\u001b[0m     \u001b[0marg_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbase_arg_names\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmissing_arg_names\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3064\u001b[0m     graph_function = ConcreteFunction(\n\u001b[1;32m-> 3065\u001b[1;33m         func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[0;32m   3066\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3067\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\uber\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m    984\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    985\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 986\u001b[1;33m       \u001b[0mfunc_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    987\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    988\u001b[0m       \u001b[1;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\uber\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    598\u001b[0m         \u001b[1;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    599\u001b[0m         \u001b[1;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 600\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    601\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mref\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    602\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\uber\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    971\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    972\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ag_error_metadata\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 973\u001b[1;33m               \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    974\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    975\u001b[0m               \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    c:\\users\\uber\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1224 test_function  *\n        return step_function(self, iterator)\n    c:\\users\\uber\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1215 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    c:\\users\\uber\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1211 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    c:\\users\\uber\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2585 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    c:\\users\\uber\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2945 _call_for_each_replica\n        return fn(*args, **kwargs)\n    c:\\users\\uber\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1208 run_step  **\n        outputs = model.test_step(data)\n    c:\\users\\uber\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1176 test_step\n        self.compiled_loss(\n    c:\\users\\uber\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\compile_utils.py:204 __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    c:\\users\\uber\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\keras\\losses.py:149 __call__\n        losses = ag_call(y_true, y_pred)\n    c:\\users\\uber\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\keras\\losses.py:253 call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    c:\\users\\uber\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    c:\\users\\uber\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\keras\\losses.py:1535 categorical_crossentropy\n        return K.categorical_crossentropy(y_true, y_pred, from_logits=from_logits)\n    c:\\users\\uber\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    c:\\users\\uber\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py:4687 categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n    c:\\users\\uber\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_shape.py:1134 assert_is_compatible_with\n        raise ValueError(\"Shapes %s and %s are incompatible\" % (self, other))\n\n    ValueError: Shapes (None, 16) and (None, 10) are incompatible\n"
     ]
    }
   ],
   "source": [
    "# loading json and creating model\n",
    "from keras.models import model_from_json\n",
    "json_file = open('model1.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"saved_models/Emotion_Voice_Detection_Model1.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    "\n",
    "# evaluate loaded model on test data\n",
    "loaded_model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "score = loaded_model.evaluate(x_testcnn, y_test, verbose=0)\n",
    "print(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}